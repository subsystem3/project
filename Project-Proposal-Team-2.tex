\documentclass{article}
\usepackage[sfdefault]{FiraSans}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{float}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hanging}
\usepackage{hyperref}
\usepackage{lipsum}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{pdfpages}
\usepackage{pgfgantt}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{xcolor}

% page layout
\geometry{ letterpaper, left=1in, right=1in, top=1in, bottom=1in, }

% color scheme
\definecolor{primary}{RGB}{0,120,215}
\definecolor{secondary}{RGB}{255,87,34}
\definecolor{background}{RGB}{245,245,245}
\definecolor{blue}{RGB}{0,62,126}

% page color
\pagecolor{background}

% hyperlink colors
\hypersetup{ colorlinks=true, linkcolor=primary, filecolor=secondary,
    urlcolor=primary, }

% line spacing
\setstretch{1.15}

% indent each reference by 0.5in on the second line
\setlength{\bibhang}{0.5in}

% no paragraph indentation
\setlength{\parindent}{0pt}

\begin{document}

\newcommand{\mytitlepage}[2]{
    \thispagestyle{empty}

    \begin{tikzpicture}[remember picture, overlay]
        \node [inner sep=0pt] at (current page.center) {#1}; { \node [
            anchor=center, inner sep=1.25cm, rectangle, fill=blue!70!white, fill opacity=0,
            text opacity=1, minimum height=0.2\paperheight, minimum width=\paperwidth, text
            width=0.8\paperwidth, font=\fontfamily{pnc}\selectfont ] at (current
        page.center) {#2}; } \node [anchor=south east, outer sep=3pt] at (current
        page.south east) {\includegraphics[width=0.33\paperwidth]{images/logo.png}};
    \end{tikzpicture}

    \newpage
}

{ \mytitlepage{\includegraphics[width=\paperwidth]{images/background.png}} {
        \centering
        \fontfamily{phv}

        \vspace{-200pt} % move title up

        { \Huge
            \bfseries

            Real-Time Brand Sentiment Analysis of Social Media Text Content

            \par
        }

        \vspace{8pt}

        { \Large
            \bfseries

            Proposal

            \par
        }

        \vspace{24pt}

        {
            \begin{center}
                \begin{tabular*} {\textwidth}{@{\extracolsep{\fill}}c c c}
                    {\LARGE Jonathan Agustin} & {\LARGE Fernando Calderon} & {\LARGE Juliet Lawton}
                \end{tabular*}
            \end{center}
        } } }

\pagenumbering{arabic}

\begin{abstract}

    \noindent Businesses increasingly leverage social media platforms to track
    brand perception using Natural Language Processing (NLP), Classification, and
    Sentiment Analysis. Machine Learning (ML) models are trained with a large
    corpus of social media text data using Deep Learning to classify a brand and
    categorize sentiment towards the brand as positive, negative, or neutral.
    However, existing systems are either inaccessible or limited in ability. We
    introduce an automated system that provides real-time insight on brand
    sentiment, enabling quick reaction to shifts in public sentiment. The proposed
    system enhances the ability to maintain a positive brand image and stay ahead
    of the competition.

\end{abstract}

\section*{Problem}

Given a set $B$ of strings representing brand names $B = \{b_1 =
    \texttt{"Nike"}, b_2 = \texttt{"Google"}, b_3 = \texttt{"Disney"}, \ldots,
    b_i\}$ and a set $P$ of strings representing social media posts
\begin{align*}
    p_1 & = \texttt{"Celebrating my birthday"}                      \\
    p_2 & = \texttt{"I could care less about the new Air Force 1s"} \\
    p_3 & = \texttt{"Happiest place on Earth"}                      \\
        & \vdots                                                    \\
    p_j & \in P
\end{align*}

the goal is to build a $\mathrm{BrandClassifier}(p)$ model that predicts
whether a social media post $p$ expresses a brand, and returns a brand name $b$
when the post expresses the brand, or $\texttt{No-Brand}$ when the post does
not express any brand. The model also returns a confidence score $c \in [0, 1]$
that indicates how confident the model is in its prediction.

\[
    \mathrm{BrandClassifier}(p) \rightarrow \begin{cases}
        (b, c)                 & \text{if } p \text{ expresses a brand } b \in
        B
        \\
        (\texttt{No-Brand}, c) & \text{otherwise}
    \end{cases}
\]

We also build a $\mathrm{BrandSentimentAnalyzer}(p, b)$ model that predicts the
sentiment of a social media post towards a brand, and returns a sentiment label

\[
    s \in S =
    \{
    s^{+}=\texttt{Positive},
    s^{-}=\texttt{Negative},
    s^{0}=\texttt{Neutral},
    s^{?}=\texttt{Mixed}
    \}
\]

when the post expresses sentiment towards the brand, or $\texttt{No-Sentiment}$
when it does not express any sentiment. The model also returns a confidence
score $c \in [0, 1]$.

\[
    \mathrm{BrandSentimentAnalyzer}(p, b) \rightarrow \begin{cases}
        (s, c)                     & \text{if } p \text{ expresses sentiment }
        s \in S \text{ about brand } b \in B
        \\
        (\texttt{No-Sentiment}, c) & \text{otherwise}
    \end{cases}
\]
\textbf{Example.} Given the following social media post $p_3 = \texttt{"I could
        care less about the new Air Force 1s"}$ and $b_3 = \texttt{"Nike"}$:
\begin{align*}
     & \mathrm{BrandClassifier}(p_3) \rightarrow (b_3, 0.68)               \\
     & \mathrm{BrandSentimentAnalyzer}(p_3, b_3) \rightarrow (s^{-}, 0.82)
\end{align*}

\newpage

\section*{Methodology}

The work is divided into four stages: assessment, development, deployment, and
evaluation. Each stage informs and refines previous stages, creating a feedback
loop that continuously improves the overall system.

\begin{enumerate}[leftmargin=*]

    \item \textbf{Assessment.} The stage involves selecting and testing
          appropriate models and datasets. The specific requirements of the sentiment
          analysis task, the characteristics of the available data, and the nature of the
          problem guide this selection process. The chosen models must effectively handle
          the unstructured and noisy data typical of social media and accurately capture
          and predict sentiments. The selected datasets must accurately represent the
          social media environment and provide a diverse view of expressed sentiments.

    \item \textbf{Development.} This stage preprocesses data in real-time to
          simulate a live data stream and training the selected models. The unstructured
          and noisy nature of social media data necessitates the transformation of text
          into a structured format suitable for modeling. This process involves handling
          invalid values and discarding non-textual content such as images, videos, or
          audio. Automation ensures efficiency in this process. The selected models are
          then adapted to the preprocessed dataset and trained to identify a brand or
          align with the sentiment expressed in the social media text.

    \item \textbf{Deployment.} This stage deploys the models to process social
          media data in near real-time. This stage requires building infrastructure to
          support the models and integrating the models with social media platforms such
          as Twitter where data will be streamed.

    \item \textbf{Evaluation.} This stage measures the models' ability to
          identify brands and predict sentiment in social media. For a time period,
          social media posts are collected and manually labeled for brand and sentiment.
          Standard classification metrics such as Accuracy, Precision, Recall, and F1
          Score are used to evaluate the models. The models are then ranked by
          performance.

\end{enumerate}

\section*{Expected Behaviors}
We expect our models to demonstrate a nuanced understanding of the English
language, accurately identifying brands and sentiments. Despite the inherent
challenges associated with sentiment analysis and brand identification in text
data, we anticipate that our models will achieve an accuracy rate exceeding
50\%. We foresee our models to have real-world applicability, serving as a
valuable tool for businesses to accurately analyze sentiment from social media
data. The proposed system is expected to provide actionable insights that
enable companies to make informed decisions.\\
\\
\textbf{Limitations.} The model's primary limitation is its unimodal nature. It
processes only text data, ignoring non-textual elements like images, videos,
and audio clips. Often found in social media posts, these elements can contain
significant sentiment-related information. This information can support,
contradict, or add nuance to the sentiment expressed in the text. Audio clips,
for example, can provide sentiment-related cues through tone, pitch, and other
features. The proposed system, however, will identify this multimodal
information and discard the input completely. Future work should consider
expanding the model to a multimodal framework to provide a more comprehensive
sentiment analysis of social media posts.

\newpage

\begin{thebibliography}{}

    \bibitem[Devlin et al., 2018]{devlin2018bert}
    Devlin, J., Chang, M. W., Lee, K., \& Toutanova, K. (2018). \newblock BERT:
    Pre-training of Deep Bidirectional Transformers for Language Understanding.
    \newblock {\em arXiv preprint arXiv:1810.04805}.

    \bibitem[Liu et al., 2019]{liu2019roberta}
    Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., \ldots \&
    Stoyanov, V. (2019). \newblock RoBERTa: A Robustly Optimized BERT Pretraining
    Approach. \newblock {\em arXiv preprint arXiv:1907.11692}.

    \bibitem[Radford et al., 2019]{radford2019language}
    Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., \& Sutskever, I.
    (2019). \newblock Language Models are Unsupervised Multitask Learners.
    \newblock {\em OpenAI Blog, 1(8)}.

    \bibitem[Sanh et al., 2019]{sanh2019distilbert}
    Sanh, V., Debut, L., Chaumond, J., \& Wolf, T. (2019). \newblock
    DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.
    \newblock {\em arXiv preprint arXiv:1910.01108}.

    \bibitem[Hutto \& Gilbert, 2014]{hutto2014vader}
    Hutto, C.J., \& Gilbert, E. (2014). \newblock VADER: A Parsimonious
    Rule-based Model for Sentiment Analysis of Social Media Text. \newblock {\em
        Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann
        Arbor, MI, June 2014}.

\end{thebibliography}

\thispagestyle{empty}

\end{document}
