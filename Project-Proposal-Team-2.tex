\documentclass{article}
\usepackage[sfdefault]{FiraSans}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{float}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hanging}
\usepackage{hyperref}
\usepackage{lipsum}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{pdfpages}
\usepackage{pgfgantt}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{xcolor}

% page layout
\geometry{ letterpaper, left=1in, right=1in, top=1in, bottom=1in, }

% color scheme
\definecolor{primary}{RGB}{0,120,215}
\definecolor{secondary}{RGB}{255,87,34}
\definecolor{background}{RGB}{245,245,245}
\definecolor{blue}{RGB}{0,62,126}

% page color
\pagecolor{background}

% hyperlink colors
\hypersetup{ colorlinks=true, linkcolor=primary, filecolor=secondary,
    urlcolor=primary, }

% line spacing
\setstretch{1.15}

% indent each reference by 0.5in on the second line
\setlength{\bibhang}{0.5in}

% no paragraph indentation
\setlength{\parindent}{0pt}

\begin{document}

\newcommand{\mytitlepage}[2]{
    \thispagestyle{empty}

    \begin{tikzpicture}[remember picture, overlay]
        \node [inner sep=0pt] at (current page.center) {#1}; { \node [
            anchor=center, inner sep=1.25cm, rectangle, fill=blue!70!white,
            fill opacity=0,
            text opacity=1, minimum height=0.2\paperheight, minimum
            width=\paperwidth, text
            width=0.8\paperwidth, font=\fontfamily{pnc}\selectfont ] at
        (current
        page.center) {#2}; } \node [anchor=south east, outer sep=3pt] at
        (current
        page.south east)
        {\includegraphics[width=0.33\paperwidth]{images/logo.png}};
    \end{tikzpicture}

    \newpage
}

{ \mytitlepage{\includegraphics[width=\paperwidth]{images/background.png}} {
        \centering
        \fontfamily{phv}

        \vspace{-200pt} % move title up

        { \Huge
            \bfseries

            Real-Time Brand Sentiment Analysis of Social Media Text Content

            \par
        }

        \vspace{8pt}

        { \Large
            \bfseries

            Proposal

            \par
        }

        \vspace{24pt}

        {
            \begin{center}
                \begin{tabular*} {\textwidth}{@{\extracolsep{\fill}}c c c}
                    {\LARGE Jonathan Agustin} & {\LARGE Fernando Calderon} &
                    {\LARGE Juliet Lawton}
                \end{tabular*}
            \end{center}
        } } }

\pagenumbering{arabic}

\section*{Introduction}

\noindent Businesses increasingly leverage social media platforms to track
brand perception using Natural Language Processing (NLP), Classification, and
Sentiment Analysis. Machine Learning (ML) models are trained with a large
corpus of social media text data using Deep Learning to classify a brand and
categorize sentiment towards the brand as positive, negative, or neutral. The
objective of this project is to train a ML model on the Sentiment140 dataset to
classify the sentiment of tweets, allowing brands to quickly react to shifts in
public sentiment and maintain a positive brand image. We will experiment with a
variety of different ML algorithms, such as Naïve Bayes and Logistic
Regression, to find the best fit.

\section*{Problem}

Given a set $B$ of strings representing brand names $B = \{b_1 =
    \texttt{"Nike"}, b_2 = \texttt{"Google"}, b_3 = \texttt{"Disney"}, \ldots,
    b_i\}$ and a set $P$ of strings representing Twitter posts
\begin{align*}
    p_1 & = \texttt{"Celebrating my birthday"}                      \\
    p_2 & = \texttt{"I could care less about the new Air Force 1s"} \\
    p_3 & = \texttt{"Happiest place on Earth"}                      \\
        & \vdots                                                    \\
    p_j & \in P
\end{align*}

the goal is to build a $\mathrm{BrandClassifier}(p)$ model that predicts
whether a
Twitter post $p$ expresses a brand, and returns a brand name $b$ when the
post expresses the brand, or $\texttt{No-Brand}$ when the post does not express
any brand. The model also returns a confidence score $c \in [0, 1]$ that
indicates how confident the model is in its prediction.

\[
    \mathrm{BrandClassifier}(p) \rightarrow \begin{cases}
        (b, c)                 & \text{if } p \text{ expresses a brand } b \in
        B
        \\
        (\texttt{No-Brand}, c) & \text{otherwise}
    \end{cases}
\]

We also build a $\mathrm{BrandSentimentAnalyzer}(p, b)$ model that predicts
the sentiment of a Twitter post towards a brand, and returns a sentiment
label

\[
    s \in S =
    \{
    s^{+}=\texttt{Positive},
    s^{-}=\texttt{Negative},
    s^{0}=\texttt{Neutral},
    s^{?}=\texttt{Mixed}
    \}
\]

when the post expresses sentiment towards the brand, or $\texttt{No-Sentiment}$
when it does not express any sentiment. The model also returns a confidence
score $c \in [0, 1]$.

\[
    \mathrm{BrandSentimentAnalyzer}(p, b) \rightarrow \begin{cases}
        (s, c)                     & \text{if } p \text{ expresses sentiment }
        s \in S \text{ about brand } b \in B
        \\
        (\texttt{No-Sentiment}, c) & \text{otherwise}
    \end{cases}
\]
\textbf{Example.} Given the following Twitter post $p_3 = \texttt{"I could
        care less about the new Air Force 1s"}$ and $b_3 = \texttt{"Nike"}$:
\begin{align*}
     & \mathrm{BrandClassifier}(p_3) \rightarrow (b_3, 0.68)               \\
     & \mathrm{BrandSentimentAnalyzer}(p_3, b_3) \rightarrow (s^{-}, 0.82)
\end{align*}

For this problem, the primary machine learning algorithms for consideration are
Naïve Bayes, Logistic Regression, Support Vector Machines, Recurrent Neural
Networks, and Transformers (such as BERT).

\section*{Related Course Topics}

This project will cover a variety of topics from AAI 501, patricularly Natural
Language Processing and Classification. More topics may also be covered
depending on which model is selected. The potential topics covered include:
\begin{enumerate}[leftmargin=*]

    \item{Natural Language Processing}
    \item{Classification}
    \item{Regression}
    \item{Supervised Machine Learning}
    \item{Bayesian Networks}
    \item{Deep Learning}

\end{enumerate}

\section*{Methodology}

The work is divided into three stages: assessment, development, and
evaluation. Each stage informs and refines previous stages, creating a feedback
loop that continuously improves the overall system.

\begin{enumerate}[leftmargin=*]

    \item \textbf{Assessment.} The stage involves analyzing the dataset and
          selecting and testing
          appropriate models. The specific requirements of the
          sentiment
          analysis task, the characteristics of the available data, and the
          nature of the
          problem guide this selection process.

    \item \textbf{Development.} This stage involves training and tuning the
          selected models. This process involves
          handling
          invalid values and discarding non-textual content such as images,
          videos, or
          audio. Automation ensures efficiency in this process. The selected
          models are
          trained to identify a
          brand or
          align with the sentiment expressed in a tweet, and then tuned to
          improve performance.

    \item \textbf{Evaluation.} This stage measures the models' ability to
          identify brands and predict sentiment in Twitter posts. The tweets
          used for validation can come	     from a test set partitioned from
          the
          original dataset, as well as from the Twitter API.
          Standard classification metrics such as Accuracy, Precision, Recall,
          and F1
          Score are used to evaluate the models. The models are then ranked by
          performance.

\end{enumerate}

\section*{Expected Behaviors}
We expect our models to demonstrate a nuanced understanding of the English
language, accurately identifying brands and sentiments. Despite the inherent
challenges associated with sentiment analysis and brand identification in text
data, we anticipate that our models will achieve an accuracy rate exceeding
50\%. We foresee our models to have real-world applicability, serving as a
valuable tool for businesses to accurately analyze sentiment from social media
data. The proposed system is expected to provide actionable insights that
enable companies to make informed decisions.\\
\\
\textbf{Known Challenges} A well-known problem in sentiment analysis is the
"aboutness" problem. This problem refers to the challenge of determining the
subject of the sentiment expressed in a sentence. For our scenario, this may
manifest in one of two ways - complex sentences and ambiguous subjects. In the
first instance, take for example the sentence "I love Disneyland but hate how
expensive the tickets are." This sentence expresses both positive and negative
sentiment towards the brand, Disney, but the overall sentiment is positive. In
the second scenario, take for example the sentences "Google is a great company"
and "You should google that." The first sentence refers to Google the brand
with positive sentiment, and the second sentence refers to google as a verb and
should not be regarded as an expression of sentiment towards the brand. \\

\textbf{Limitations} The primary limitation of the model is that it is trained
on Twitter posts, so it is not yet able to classify posts from other social
media sites, such as Facebook. Another limitation is the unimodal nature of the
model. It processes only text data, ignoring non-textual elements like images,
videos,
and audio clips. Often found in social media posts, these elements can contain
significant sentiment-related information. This information can support,
contradict, or add nuance to the sentiment expressed in the text. Audio clips,
for example, can provide sentiment-related cues through tone, pitch, and other
features. Future work should consider generalizing the model to work with other
kinds of social media posts, as well as
expanding the model to a multimodal framework to provide a more comprehensive
sentiment analysis of social media posts.

\newpage

\begin{thebibliography}{}

    \bibitem[Devlin et al., 2018]{devlin2018bert}
    Devlin, J., Chang, M. W., Lee, K., \& Toutanova, K. (2018). \newblock BERT:
    Pre-training of Deep Bidirectional Transformers for Language Understanding.
    \newblock {\em arXiv preprint arXiv:1810.04805}.

    \bibitem[Liu et al., 2019]{liu2019roberta}
    Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., \ldots \&
    Stoyanov, V. (2019). \newblock RoBERTa: A Robustly Optimized BERT
    Pretraining
    Approach. \newblock {\em arXiv preprint arXiv:1907.11692}.

    \bibitem[Radford et al., 2019]{radford2019language}
    Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., \& Sutskever, I.
    (2019). \newblock Language Models are Unsupervised Multitask Learners.
    \newblock {\em OpenAI Blog, 1(8)}.

    \bibitem[Sanh et al., 2019]{sanh2019distilbert}
    Sanh, V., Debut, L., Chaumond, J., \& Wolf, T. (2019). \newblock
    DistilBERT, a distilled version of BERT: smaller, faster, cheaper and
    lighter.
    \newblock {\em arXiv preprint arXiv:1910.01108}.

    \bibitem[Hutto \& Gilbert, 2014]{hutto2014vader}
    Hutto, C.J., \& Gilbert, E. (2014). \newblock VADER: A Parsimonious
    Rule-based Model for Sentiment Analysis of Social Media Text. \newblock
    {\em
        Eighth International Conference on Weblogs and Social Media (ICWSM-14).
        Ann
        Arbor, MI, June 2014}.

\end{thebibliography}

\thispagestyle{empty}

\end{document}