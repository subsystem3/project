{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<https://www.analyticsvidhya.com/blog/2021/12/sentiment-analysis-on-tweets-with-lstm-for-beginners/>\n",
    "\n",
    "<https://www.kaggle.com/code/stoicstatic/twitter-sentiment-analysis-for-beginners>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "SENTIMENT140_TRAIN = \"datasets/training.1600000.processed.noemoticon.csv\"\n",
    "SENTIMENT140_FIELDS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "\n",
    "class Tex(Enum):\n",
    "    SECTION = 1\n",
    "    SUBSECTION = 2\n",
    "    SUBSUBSECTION = 3\n",
    "    SLIDE = 4\n",
    "    TEXT = 5\n",
    "    IMAGE = 6\n",
    "\n",
    "\n",
    "# HOLD CONTENT FOR GENERATING REPORT\n",
    "class Tex(Enum):\n",
    "    SECTION = 1\n",
    "    SUBSECTION = 2\n",
    "    SUBSUBSECTION = 3\n",
    "    SLIDE = 4\n",
    "    TEXT = 5\n",
    "    IMAGE = 6\n",
    "\n",
    "\n",
    "class ContentList(list):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        self.index = 0\n",
    "\n",
    "    def append(self, tex: Union[Tex, str], header: str, content: str):\n",
    "        self.append((self.index, tex, header, content))\n",
    "        self.index += 1\n",
    "\n",
    "\n",
    "content: ContentList = ContentList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git LFS\n",
    "\n",
    "Pull the datasets from Git LFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs pull -I \"datasets/training.1600000.processed.noemoticon.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import, Initialize, & Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "`python-dotenv` is used to load environment variables from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy\n",
    "\n",
    "`numpy` is used for numerical processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn\n",
    "\n",
    "`seaborn` is used to set the default style for plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib\n",
    "\n",
    "`matplotlib` is used to plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "`pandas` is a Python library for data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights & Biases\n",
    "\n",
    "[Weights & Biases](https://www.wandb.com/) is used for tracking and visualizing experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjagustin\u001b[0m (\u001b[33msubsystem3\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/user/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.finish()\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ContentList.append() missing 2 required positional arguments: 'header' and 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## Exploratory Data Analysis (EDA)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m content\u001b[39m.\u001b[39;49mappend((Tex\u001b[39m.\u001b[39;49mSECTION, \u001b[39m\"\u001b[39;49m\u001b[39mExploratory Data Analysis\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "\u001b[0;31mTypeError\u001b[0m: ContentList.append() missing 2 required positional arguments: 'header' and 'content'"
     ]
    }
   ],
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "content.append((Tex.SECTION, \"Exploratory Data Analysis\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"\\href{http://help.sentiment140.com/}{Sentiment140} is a dataset containing 1.6 million tweets with sentiment labels. \n",
    "\n",
    "The dataset has 6 fields:\n",
    "\n",
    "\\begin{table}[h]\n",
    "    \\centering\n",
    "        \\begin{tabular}{|c|l|}\n",
    "            \\hline\n",
    "            \\textbf{Field} & \\textbf{Description} \\\\\n",
    "            \\hline\n",
    "            target & The polarity of the tweet (0 = negative, 2 = neutral, 4 = positive) \\\\\n",
    "            id & The id of the tweet (2087) \\\\\n",
    "            date & The date of the tweet (Sat May 16 23:58:44 UTC 2009) \\\\\n",
    "            flag & The query (lyx). If there is no query, then this value is NO\\_QUERY. \\\\\n",
    "            user & The user that tweeted (robotickilldozr) \\\\\n",
    "            text & The text of the tweet (Lyx is cool) \\\\\n",
    "            \\hline\n",
    "        \\end{tabular}\n",
    "    \\caption{Description of items}\n",
    "    \\label{tab:my_label}\n",
    "\\end{table}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "content.append((Tex.SUBSECTION, f\"Sentiment140 Dataset\", raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "    SENTIMENT140_TRAIN,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    names=SENTIMENT140_FIELDS,\n",
    "    header=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"df_train.shape: {df_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = f\"\"\"We show that the dataset is balanced by counting the number of positive and negative tweets. Balance is important because it means that the model will be trained on an equal number of positive and negative tweets. If the dataset was imbalanced, then the model would be trained on more of one class than the other. This would result in a model that is biased towards the class with more samples.\n",
    "\"\"\"\n",
    "\n",
    "content.append((Tex.SUBSECTION, f\"Class Balance\", raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW SENTIMENT DISTRIBUTION IN TRAINING SET\n",
    "distribution = df_train[\"target\"].value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=distribution.index, y=distribution.values)\n",
    "plt.title(\"Distribution of Sentiment in Sentiment140 Training Set\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks([0, 1], [\"Negative\", \"Positive\"])\n",
    "\n",
    "# ADD COUNTS ABOVE THE BARS\n",
    "for i, v in enumerate(distribution.values):\n",
    "    plt.text(i, v, str(v), ha=\"center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(\n",
    "    df: pd.DataFrame, column: str, filename: str, directory: str = \"wordclouds\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate a wordcloud from a column in a dataframe.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Filename to save the wordcloud as.\n",
    "        df (pd.DataFrame): Dataframe.\n",
    "        column (str): Column name.\n",
    "\n",
    "    \"\"\"\n",
    "    from wordcloud import WordCloud\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        width=1600,\n",
    "        height=800,\n",
    "        background_color=\"white\",\n",
    "        min_font_size=10,\n",
    "        max_words=1000,\n",
    "        collocations=False,\n",
    "    )\n",
    "\n",
    "    wordcloud.generate(\" \".join(df[column].tolist()))\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    output_path = os.path.join(directory, filename)\n",
    "    plt.savefig(output_path, format=\"png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "generate_wordcloud(df_train, \"text\", \"wordcloud_train.png\", directory=\"report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is traditionally an important step for Natural Language Processing (NLP) tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.\n",
    "\n",
    "Preprocessing steps include:\n",
    "\n",
    "* Lower Casing: Each text is converted to lowercase.\n",
    "* Replacing URLs: Links starting with \"http\" or \"https\" or \"www\" are replaced by \"URL\".\n",
    "* Replacing Emojis: Replace emojis by using a pre-defined dictionary containing emojis along with their meaning. (eg: \":)\" to \"EMOJIsmile\")\n",
    "* Replacing Usernames: Replace @Usernames with word \"USER\". (eg: \"@Kaggle\" to \"USER\")\n",
    "* Removing Non-Alphabets: Replacing characters except Digits and Alphabets with a space.\n",
    "* Removing Consecutive letters: 3 or more consecutive letters are replaced by 2 letters. (eg: \"Heyyyy\" to \"Heyy\")\n",
    "* Removing Short Words: Words with length less than 2 are removed.\n",
    "* Removing Stopwords: Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. (eg: \"the\", \"he\", \"have\")\n",
    "* Lemmatizing: Lemmatization is the process of converting a word to its base form. (e.g: \"Great\" to \"Good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE UNNECESSARY COLUMNS\n",
    "df_train = df_train[[\"text\", \"target\"]]\n",
    "\n",
    "# Re-map sentiment labels to be 0=negative and 1=positive (instead of of 4=positive)\n",
    "df_train[\"target\"] = df_train[\"target\"].replace(4, 1)\n",
    "\n",
    "# CHECK\n",
    "df_train[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords like \"the\", \"a\", and \"is\" are so common that they are not useful for training the model.\n",
    "\n",
    "The `nltk` library is used to remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import nltk\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# corpus = []\n",
    "# for i in range(0, int(df_train.shape[0])):\n",
    "#     text = re.sub(\"[^a-zA-z]\", \" \", df_train[\"text\"][i])\n",
    "#     text = text.lower()\n",
    "#     text = text.split()\n",
    "#     ps = PorterStemmer()\n",
    "#     all_stopwords = stopwords.words(\"english\")\n",
    "#     all_stopwords.remove(\"not\")\n",
    "#     text = [ps.stem(word) for word in text if not word in set(all_stopwords)]\n",
    "#     text = \" \".join(text)\n",
    "#     corpus.append(text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Stemming is used to reduce words to their root form. We do this because many words have the same meaning but are written differently.\n",
    "For example, \"run\", \"runs\", and \"running\" all have the same meaning. This will reduce the number of unique words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: STEMMING/LEMMATIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train[\"text\"], df_train[\"target\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectoriser\n",
    "\n",
    "TF-IDF indicates what the importance of the word is in order to understand the document or dataset. Let us understand with an example. Suppose you have a dataset where students write an essay on the topic, My House. In this dataset, the word a appears many times; itâ€™s a high frequency word compared to other words in the dataset. The dataset contains other words like home, house, rooms and so on that appear less often, so their frequency are lower and they carry more information compared to the word. This is the intuition behind TF-IDF.\n",
    "\n",
    "TF-IDF Vectoriser converts a collection of raw documents to a matrix of TF-IDF features. The Vectoriser is usually trained on only the X_train dataset.\n",
    "\n",
    "ngram_range is the range of number of words in a sequence. [e.g \"very expensive\" is a 2-gram that is considered as an extra feature separately from \"very\" and \"expensive\" when you have a n-gram range of (1,2)]\n",
    "\n",
    "max_features specifies the number of features to consider. [Ordered by feature frequency across the corpus]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1, 2), max_features=500000)\n",
    "vectoriser.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranforming the dataset\n",
    "\n",
    "Transforming the X_train and X_test dataset into matrix of TF-IDF Features by using the TF-IDF Vectoriser. This datasets will be used to train the model and test against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectoriser.transform(X_train)\n",
    "X_test = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import joblib\n",
    "\n",
    "\n",
    "def model_Evaluate(model):\n",
    "    with wandb.init(\n",
    "        project=\"project\",\n",
    "        config={\"model\": model},\n",
    "        name=f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}_{model}\",\n",
    "    ) as run:\n",
    "        # PREDICT VALUES FROM TEST DATASET\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # PRINT THE CLASSIFICATION REPORT\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # COMPUTE AND PLOT CONFUSION MATRIX\n",
    "        cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        categories = [\"Negative\", \"Positive\"]\n",
    "        group_names = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"]\n",
    "        group_percentages = [\n",
    "            \"{0:.2%}\".format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)\n",
    "        ]\n",
    "        labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names, group_percentages)]\n",
    "        labels = np.asarray(labels).reshape(2, 2)\n",
    "        sns.heatmap(\n",
    "            cf_matrix,\n",
    "            annot=labels,\n",
    "            cmap=\"Blues\",\n",
    "            fmt=\"\",\n",
    "            xticklabels=categories,\n",
    "            yticklabels=categories,\n",
    "        )\n",
    "        plt.xlabel(\"Predicted values\", fontdict={\"size\": 14}, labelpad=10)\n",
    "        plt.ylabel(\"Actual values\", fontdict={\"size\": 14}, labelpad=10)\n",
    "        plt.title(\"Confusion Matrix\", fontdict={\"size\": 18}, pad=20)\n",
    "\n",
    "        # LOG TO W&B\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "                \"precision\": precision_score(\n",
    "                    y_test, y_pred, average=\"weighted\", zero_division=1\n",
    "                ),\n",
    "                \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "                \"f1\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
    "                \"roc_auc\": roc_auc_score(y_test, y_pred),\n",
    "                \"confusion_matrix\": wandb.Image(plt),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # LOG MODEL\n",
    "        artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "        joblib.dump(model, \"model.pkl\")\n",
    "        artifact.add_file(\"model.pkl\")\n",
    "        run.log_artifact(artifact)\n",
    "        os.remove(\"model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "BNBmodel = BernoulliNB(alpha=2)\n",
    "BNBmodel.fit(X_train, y_train)\n",
    "model_Evaluate(BNBmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "SVCmodel = LinearSVC(dual=False)\n",
    "SVCmodel.fit(X_train, y_train)\n",
    "model_Evaluate(SVCmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LRmodel = LogisticRegression(C=2, max_iter=1000, n_jobs=-1)\n",
    "LRmodel.fit(X_train, y_train)\n",
    "model_Evaluate(LRmodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
