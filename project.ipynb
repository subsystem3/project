{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brand Sentiment Analysis of Twitter Posts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PULL FROM GIT LARGE FILE STORAGE (LFS)\n",
    "# !git lfs pull --include=\"datasets/*\"\n",
    "!git lfs pull --include=\"cache/*\"\n",
    "!git lfs pull --include=\"*.mp4\"\n",
    "!git lfs pull --include=\"Final-Project-Team-2.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Management\n",
    "\n",
    "We define a list of required packages and a function for installing them. This function, `handle_packages`, checks for each package in the system. If a package is not found, it is installed quietly using the pip package manager. \n",
    "\n",
    "By invoking `handle_packages` with the list of required packages, we make sure all project dependencies are satisfied, setting up a reliable and reproducible environment for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# INSTALL REQUIRED PACKAGES FOR PROJECT\n",
    "PACKAGES = [\n",
    "    \"adjustText\",\n",
    "    \"colorama\",\n",
    "    \"dotenv\",\n",
    "    \"ipython\",\n",
    "    \"joblib\",\n",
    "    \"matplotlib\",\n",
    "    \"nltk\",\n",
    "    \"numpy\",\n",
    "    \"openai\",\n",
    "    \"pandas\",\n",
    "    \"preprocessor\",\n",
    "    \"python-dotenv\",\n",
    "    \"seaborn\",\n",
    "    \"sklearn\",\n",
    "    \"tqdm\",\n",
    "    \"tweet-preprocessor\",\n",
    "    \"wandb\",\n",
    "    \"wordcloud\",\n",
    "]\n",
    "\n",
    "\n",
    "def handle_packages(packages: list) -> None:\n",
    "    \"\"\"Quietly installs a package if it is not already installed.\n",
    "\n",
    "    Args:\n",
    "        packages (list): The list of packages to install.\n",
    "\n",
    "    Raises:\n",
    "        ImportError: If the package is not installed, install it.\n",
    "    \"\"\"\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            os.system(f\"python3 -m pip install -q {package}\")\n",
    "\n",
    "\n",
    "handle_packages(PACKAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from queue import Queue\n",
    "from enum import Enum\n",
    "from functools import wraps\n",
    "from pprint import pprint\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import dotenv\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import preprocessor\n",
    "import seaborn as sns\n",
    "from adjustText import adjust_text\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import Markdown, clear_output, display, display_html\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import wandb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PROJECT VISUALIZATIONS/ARTIFACTS IN THE REPORT DIRECTORY\n",
    "os.makedirs(\"report\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    \"\"\"\n",
    "    A class to hold metrics for monitoring performance.\n",
    "\n",
    "    This class is a singleton, meaning that there is only one instance of the class throughout the entire program.\n",
    "    All functions decorated with @measure_time share the same Metrics object.\n",
    "\n",
    "    Attributes:\n",
    "        metrics (dict): A dictionary to store the metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    _instance = None\n",
    "\n",
    "    def __new__(cls):\n",
    "        \"\"\"\n",
    "        Overrides the default object creation method to implement the singleton pattern.\n",
    "\n",
    "        Returns:\n",
    "            Metrics: The singleton instance of the Metrics class.\n",
    "        \"\"\"\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(Metrics, cls).__new__(cls)\n",
    "            cls._instance.metrics = {}\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the Metrics class with an empty dictionary.\"\"\"\n",
    "        self.metrics: dict[str, float] = {}\n",
    "\n",
    "    def add_metric(self, key: str, value: float) -> None:\n",
    "        \"\"\"\n",
    "        Adds a new metric to the dictionary or appends to an existing one.\n",
    "\n",
    "        Args:\n",
    "            key (str): The name of the metric.\n",
    "            value (float): The value of the metric.\n",
    "        \"\"\"\n",
    "        if key in self.metrics:\n",
    "            self.metrics[key] += value\n",
    "        else:\n",
    "            self.metrics[key] = value\n",
    "\n",
    "    def get_metrics(self) -> dict[str, float]:\n",
    "        \"\"\"\n",
    "        Returns the dictionary of metrics.\n",
    "\n",
    "        Returns:\n",
    "            dict: The dictionary of metrics.\n",
    "        \"\"\"\n",
    "        return self.metrics\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Returns a string representation of the metrics.\"\"\"\n",
    "        return \"\\n\".join(f\"{k}: {v}\" for k, v in self.metrics.items())\n",
    "\n",
    "    def __getitem__(self, key: str) -> float:\n",
    "        \"\"\"\n",
    "        Returns the value of the metric with the given key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key of the metric.\n",
    "\n",
    "        Returns:\n",
    "            float: The value of the metric.\n",
    "        \"\"\"\n",
    "        return self.metrics.get(key)\n",
    "\n",
    "    def __setitem__(self, key: str, value: float) -> None:\n",
    "        \"\"\"\n",
    "        Sets the value of the metric with the given key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key of the metric.\n",
    "            value (float): The value of the metric.\n",
    "        \"\"\"\n",
    "        self.metrics[key] = value\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of metrics.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of metrics.\n",
    "        \"\"\"\n",
    "        return len(self.metrics)\n",
    "\n",
    "    def __contains__(self, key: str) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the metric with the given key exists.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key of the metric.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the metric exists, False otherwise.\n",
    "        \"\"\"\n",
    "        return key in self.metrics\n",
    "\n",
    "    def clear_metrics(self) -> None:\n",
    "        \"\"\"Clears all the metrics.\"\"\"\n",
    "        self.metrics.clear()\n",
    "\n",
    "    def remove_metric(self, key: str) -> None:\n",
    "        \"\"\"\n",
    "        Removes the metric with the given key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key of the metric.\n",
    "        \"\"\"\n",
    "        if key in self.metrics:\n",
    "            del self.metrics[key]\n",
    "\n",
    "    def update_metric(self, key: str, value: float) -> None:\n",
    "        \"\"\"\n",
    "        Updates the value of the metric with the given key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key of the metric.\n",
    "            value (float): The new value of the metric.\n",
    "        \"\"\"\n",
    "        if key in self.metrics:\n",
    "            self.metrics[key] = value\n",
    "\n",
    "    def print_metrics(self) -> None:\n",
    "        \"\"\"Prints all the metrics in a nice format.\"\"\"\n",
    "        print(\"Metrics:\")\n",
    "        sorted_metrics = sorted(\n",
    "            self.metrics.items(), key=lambda item: item[1], reverse=True\n",
    "        )\n",
    "        for key, value in sorted_metrics:\n",
    "            print(f\"  {key}: {value:.2f} seconds\")\n",
    "        print(f\"Total time: {sum(self.metrics.values()):.2f} seconds\")\n",
    "\n",
    "\n",
    "def measure_time(run_name: str):\n",
    "    \"\"\"\n",
    "    A decorator factory that measures the time taken by a function and logs it to the Metrics singleton.\n",
    "\n",
    "    Args:\n",
    "        run_name (str): The name of the run. This is used to differentiate metrics among different runs.\n",
    "\n",
    "    Returns:\n",
    "        Callable: The decorator that wraps the function with timing and logging.\n",
    "    \"\"\"\n",
    "\n",
    "    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:\n",
    "        \"\"\"\n",
    "        The actual decorator that wraps the function with timing and logging.\n",
    "\n",
    "        Args:\n",
    "            func (Callable): The function to be timed and logged.\n",
    "\n",
    "        Returns:\n",
    "            Callable: The wrapped function.\n",
    "        \"\"\"\n",
    "\n",
    "        def wrapper(*args, **kwargs):\n",
    "            \"\"\"\n",
    "            The wrapper function that adds timing and logging to the original function.\n",
    "\n",
    "            Args:\n",
    "                *args: The positional arguments for the original function.\n",
    "                **kwargs: The keyword arguments for the original function.\n",
    "\n",
    "            Returns:\n",
    "                The result of the original function.\n",
    "            \"\"\"\n",
    "            # START TIMER\n",
    "            start_time = time.time()\n",
    "            # CALL FUNCTION\n",
    "            result = func(*args, **kwargs)\n",
    "            # CALCULATE ELAPSED TIME\n",
    "            total_time = time.time() - start_time\n",
    "            # LOG RESULT\n",
    "            metrics.add_metric(f\"{run_name}\", total_time)\n",
    "            print(f\"{run_name}...{total_time:.2f} seconds\")\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "metrics = Metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Data Analysis and Visualization Tools\n",
    "\n",
    "We import data handling and visualization libraries and configure them for ideal display. These steps prepare our environment for efficient data analysis and visualization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE\n",
    "pd.set_option(\"display.max_colwidth\", 80)\n",
    "\n",
    "# SET MATPLOTLIB STYLE TO 'fivethirtyeight'\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Environment Variables\n",
    "\n",
    "We load environment variables from a `.env` file using the `dotenv` package. This file has key-value pairs of environment variables, which are then added to the system's environment variables. This approach lets us manage sensitive information securely and consistently across different project stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD ENVIRONMENT VARIABLES\n",
    "ENVIRONMENT = dotenv.dotenv_values()\n",
    "for key in ENVIRONMENT:\n",
    "    os.environ[key] = ENVIRONMENT[key]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving and Preparing the Sentiment140 Dataset\n",
    "\n",
    "The Sentiment140 dataset, available at <http://help.sentiment140.com/for-students>, is a collection of 1.6 million tweets with sentiment labels. \n",
    "\n",
    "It is used for discovering the sentiment of a brand, product, or topic on Twitter.\n",
    "\n",
    "The dataset, provided as a CSV file with emoticons removed, includes tweet content and sentiment labels. We extract these parts and remap the sentiment labels for easier interpretation in future analyses.\n",
    "\n",
    "The dataset has 6 fields:\n",
    "1. `target` — the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "1. `id` — the id of the tweet (2087)\n",
    "1. `date` — the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "1. `flag` — the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "1. `user` — the user that tweeted (robotickilldozr)\n",
    "1. `text` — the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\n",
    "    \"datasets/training.1600000.processed.noemoticon.csv\",\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    names=[\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"tweet\"],\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "# GET TWEETS AND SENTIMENT LABELS\n",
    "tweets_df = tweets_df[[\"tweet\", \"sentiment\"]]\n",
    "\n",
    "# REMAP SENTIMENT LABELS TO 0 = negative, 1 = positive (instead of 0 = negative, 4 = positive)\n",
    "tweets_df.sentiment = tweets_df.sentiment.replace(4, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Balance\n",
    "\n",
    "We ensure class balance in our dataset by counting the number of positive and negative tweets. Class balance is important because the model is trained on an equal number of instances from each class, preventing bias towards a particular sentiment. \n",
    "\n",
    "We generate a bar plot showing the distribution of sentiments in the training dataset, which indicates whether models will be trained on a balanced set of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW SENTIMENT DISTRIBUTION IN TRAINING SET\n",
    "distribution = tweets_df.sentiment.value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=distribution.index, y=distribution.values)\n",
    "plt.title(\"Distribution of Sentiment in Sentiment140 Training Set\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks([0, 1], [\"Negative\", \"Positive\"])\n",
    "\n",
    "for i, v in enumerate(distribution.values):\n",
    "    # ADD COUNTS ABOVE BARS\n",
    "    plt.text(i, v, str(v), ha=\"center\")\n",
    "\n",
    "# SAVE VISUALIZATION\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"report/sentiment_distribution.png\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud\n",
    "\n",
    "A word cloud is a visual representation where the most often occurring words are displayed larger than less common words. This visualization gives a quick understanding of the key themes or topics in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_time(\"generating word cloud from training data\")\n",
    "def generate_wordcloud(df_train):\n",
    "    \"\"\"\n",
    "    Generate a wordcloud from a dataframe.\n",
    "\n",
    "    Args:\n",
    "        df_train (DataFrame): The dataframe containing the text data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    wordcloud = WordCloud(\n",
    "        width=1600,\n",
    "        height=800,\n",
    "        background_color=\"white\",\n",
    "        min_font_size=10,\n",
    "        max_words=1000,\n",
    "        collocations=False,\n",
    "        random_state=42,  # set for idempotency\n",
    "    )\n",
    "\n",
    "    wordcloud.generate(\" \".join(df_train.tweet.tolist()))\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # SAVE VISUALIZATION\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"report/wordcloud.png\", dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "generate_wordcloud(tweets_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is a key step in Natural Language Processing (NLP) to improve the performance and accuracy of the models. The aim is to get the text data ready for machine learning algorithms. \n",
    "\n",
    "In its raw form, text data can be messy and unstructured, making it hard for a machine-learning model to understand and learn from. Preprocessing cleans up the data by removing unnecessary information like punctuation and common words that add little meaning or converting all text to lowercase so the model does not get confused by the same word in different cases. Transforming text into a clean and standardized form makes it easier for machine learning models to analyze the data and perform tasks like classification, prediction, or translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANDS = [\n",
    "    \"facebook\",\n",
    "    \"google\",\n",
    "    \"apple\",\n",
    "    \"starbucks\",\n",
    "    \"disney\",\n",
    "    \"microsoft\",\n",
    "    \"target\",\n",
    "    \"amazon\",\n",
    "    \"walmart\",\n",
    "    \"sony\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE FREQUENCY OF EACH BRAND IN THE TWEETS\n",
    "counter = Counter(\n",
    "    word\n",
    "    for tweet in tweets_df.tweet\n",
    "    for word in tweet.lower().split()\n",
    "    if word in BRANDS\n",
    ")\n",
    "\n",
    "# CREATE DATAFRAME FROM THE COUNTER\n",
    "brand_freqs_df = pd.DataFrame(\n",
    "    counter.items(), columns=[\"brand\", \"frequency\"]\n",
    ").sort_values(\"frequency\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE BRAND MENTION FREQUENCY\n",
    "\n",
    "# CREATE BARPLOT\n",
    "num_brands = len(brand_freqs_df)\n",
    "plt.figure(figsize=(10, num_brands * 0.5))\n",
    "barplot = sns.barplot(\n",
    "    x=\"frequency\", y=\"brand\", data=brand_freqs_df, order=brand_freqs_df.brand\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Brand Mention Frequency\")\n",
    "\n",
    "# ADD COUNTS AT THE END OF THE BARS\n",
    "for i, v in enumerate(brand_freqs_df.frequency):\n",
    "    barplot.text(v + 3, i + 0.25, str(v))\n",
    "\n",
    "# SAVE VISUALIZATION\n",
    "plt.tight_layout()\n",
    "barplot.figure.savefig(\"report/brand_mention_frequency.png\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Tweets to Identify Brand Mentions\n",
    "\n",
    "We analyze tweets and identify mentions of specific brands. The process begins by standardizing the text in each tweet, which aids in detecting brand names. Each tweet is analyzed for brand mentions. When a brand is detected, we store the original tweet text, a flag indicating a brand mention, and the brand name itself. Where no brand is mentioned, we record the brand name as `nobrand`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_time(\"creating brands dataframe\")\n",
    "def create_brand_dataframe(tweets_df: pd.DataFrame, brands: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a new dataframe with brand labels.\n",
    "\n",
    "    Args:\n",
    "        tweets_df (pd.DataFrame): The dataframe containing tweets and sentiments.\n",
    "        brands (List[str]): The list of brands to be searched in the tweets.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The new dataframe with brand labels.\n",
    "    \"\"\"\n",
    "    brand_rows = []\n",
    "    non_brand_counter = 0\n",
    "    brand_counter = 0\n",
    "    for tweet, sentiment in zip(tweets_df.tweet, tweets_df.sentiment):\n",
    "        tweet_tokens = tweet.lower().split()\n",
    "        brand_found = False\n",
    "\n",
    "        for word in tweet_tokens:\n",
    "            word = word.replace(\"-\", \"\")\n",
    "            for brand in BRANDS:\n",
    "                if word == brand:\n",
    "                    brand_rows.append(\n",
    "                        {\"tweet\": tweet, \"brand\": brand, \"sentiment\": sentiment}\n",
    "                    )\n",
    "                    brand_found = True\n",
    "                    brand_counter += 1\n",
    "                    break\n",
    "\n",
    "            if brand_found:\n",
    "                break\n",
    "\n",
    "        if not brand_found and non_brand_counter < brand_counter:\n",
    "            brand_rows.append(\n",
    "                {\"tweet\": tweet, \"brand\": \"nobrand\", \"sentiment\": sentiment}\n",
    "            )\n",
    "            non_brand_counter += 1\n",
    "\n",
    "    brands_df = pd.DataFrame(brand_rows, columns=[\"tweet\", \"brand\", \"sentiment\"])\n",
    "    return brands_df\n",
    "\n",
    "\n",
    "brands_df = create_brand_dataframe(tweets_df, BRANDS)\n",
    "display(brands_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT DATASET TO FILE\n",
    "print(len(brands_df))\n",
    "brands_df.to_csv(\"datasets/brands.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation Using OpenAI's GPT-4 Model\n",
    "\n",
    "We define parameters for data generation using OpenAI's GPT-4 model. The parameters include the model name, the desired dataset size, and a user prompt that outlines the required data format.\n",
    "\n",
    "The data format is: \n",
    "\n",
    "\"tweet\"|||brand-presence|||brand\n",
    "\n",
    "Here, 'tweet' represents a Twitter post, 'brand-presence' is a binary indicator (0 or 1) denoting the presence or absence of a brand in the tweet, and 'brand' is the name of a brand from a predefined list or 'nobrand' when no brand is mentioned in the tweet.\n",
    "\n",
    "The user prompt also includes examples to guide the data generation process and specifies that the dataset should have an equal number of tweets with and without brand mentions.\n",
    "\n",
    "We use OpenAI's API to generate the data. The API key is retrieved from the environment variables, and a chat completion task is created using the GPT-4 model and the defined user prompt. The generated data is then extracted from the chat completion response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='data_generator.log', level=logging.INFO)\n",
    "\n",
    "class Status(Enum):\n",
    "    NOT_STARTED = \"NOT_STARTED\"\n",
    "    QUEUED = \"QUEUED\"\n",
    "    BALANCING = \"BALANCING\"\n",
    "    BALANCED = \"BALANCED\"\n",
    "\n",
    "class JobAction(Enum):\n",
    "    GENERATE = \"GENERATE\"\n",
    "    DELETE = \"DELETE\"\n",
    "\n",
    "class Job:\n",
    "    def __init__(self, brand, sentiment, action: JobAction, count: int):\n",
    "        self.brand = brand\n",
    "        self.sentiment = sentiment\n",
    "        self.action = action\n",
    "        self.count = count\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, brands, model=\"gpt-4\", samples_per_brand=100, cache_file=\"cache/generated_data.pkl\"):\n",
    "        self.brands = brands\n",
    "        self.model = model\n",
    "        self.samples_per_brand = samples_per_brand\n",
    "        self.dataset_size = samples_per_brand * (len(brands) * 2 + 1)\n",
    "        self.dataset = []\n",
    "        self.seen_data = set()\n",
    "        self.brand_counts = {brand: {\"positive\": 0, \"negative\": 0} for brand in brands}\n",
    "        self.brand_counts[\"nobrand\"] = {\"positive\": 0, \"negative\": 0}\n",
    "        self.brand_status = {brand: Status.NOT_STARTED for brand in brands}\n",
    "        self.brand_status[\"nobrand\"] = Status.NOT_STARTED\n",
    "        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "        self.cache_file = cache_file\n",
    "        self.load_or_initialize_dataset()\n",
    "        self.delete_excess_data()\n",
    "        self.job_queue = Queue()\n",
    "\n",
    "    def add_job(self, brand, sentiment, action: JobAction):\n",
    "        count = self.calculate_job_count(brand, sentiment, action.value.lower())\n",
    "        job = Job(brand, sentiment, action, count)\n",
    "        self.job_queue.put(job)\n",
    "        logging.info(f'Job added: {job}')\n",
    "\n",
    "    def calculate_jobs(self):\n",
    "        for brand in self.brands + [\"nobrand\"]:\n",
    "            for sentiment in [\"positive\", \"negative\"]:\n",
    "                current_count = self.brand_counts[brand][sentiment]\n",
    "                target_count = self.samples_per_brand // 2\n",
    "                if current_count < target_count and self.brand_status[brand] != Status.BALANCED:\n",
    "                    self.add_job(brand, sentiment, JobAction.GENERATE)\n",
    "                elif current_count > target_count and self.brand_status[brand] != Status.BALANCED:\n",
    "                    self.add_job(brand, sentiment, JobAction.DELETE)\n",
    "        self.report_metrics()\n",
    "\n",
    "    def process_jobs(self):\n",
    "        while not self.job_queue.empty():\n",
    "            job = self.job_queue.get()\n",
    "            self.process_job(job)\n",
    "            self.job_queue.task_done()\n",
    "            if self.brand_status[job.brand] == Status.BALANCED: \n",
    "                self.job_queue.queue.remove(job)\n",
    "\n",
    "    def process_job(self, job):\n",
    "        self.update_status(job.brand, Status.BALANCING)\n",
    "        if job.action == JobAction.GENERATE:\n",
    "            if job.sentiment == \"positive\":\n",
    "                self.generate_positive_data(job.brand, job.count)\n",
    "            else:\n",
    "                self.generate_negative_data(job.brand, job.count)\n",
    "        elif job.action == JobAction.DELETE:\n",
    "            self.delete_excess_data(job.brand, job.sentiment, job.count)\n",
    "        self.delete_excess_data()\n",
    "        self.update_status(job.brand, Status.BALANCED)\n",
    "        self.report_metrics()\n",
    "\n",
    "    def calculate_job_count(self, brand, sentiment, action):\n",
    "        if action == 'generate':\n",
    "            return self.samples_per_brand // 2 - self.brand_counts[brand][sentiment]\n",
    "        elif action == 'delete':\n",
    "            return self.brand_counts[brand][sentiment] - self.samples_per_brand // 2\n",
    "\n",
    "    def generate_balanced_dataset(self):\n",
    "        self.report_metrics()  \n",
    "        self.calculate_jobs()  \n",
    "        self.process_jobs() \n",
    "\n",
    "    def update_status(self, brand, status):\n",
    "        self.brand_status[brand] = status\n",
    "        self.report_metrics()\n",
    "\n",
    "    def generate_brand_data(self, brand: str, sentiment: str, count: int = None):\n",
    "        while self.brand_counts[brand][sentiment] < self.samples_per_brand // 2:\n",
    "            user_prompt = self.create_prompt(brand, sentiment)\n",
    "            chunks = self.samples_per_brand // 50\n",
    "            for _ in range(chunks):\n",
    "                self.generate_and_add_data(user_prompt, brand, sentiment)\n",
    "                if self.brand_counts[brand][sentiment] >= self.samples_per_brand // 2:\n",
    "                    break\n",
    "            remaining = self.samples_per_brand % 50  # calculate the remaining samples\n",
    "            if remaining > 0 and self.brand_counts[brand][sentiment] < self.samples_per_brand // 2:\n",
    "                self.generate_and_add_data(user_prompt, brand, sentiment, remaining)  # generate the remaining samples\n",
    "            self.save_generated_data()\n",
    "            self.delete_excess_data()  \n",
    "            self.report_metrics(brand)\n",
    "\n",
    "    def generate_and_add_data(self, user_prompt: str, brand: str, sentiment: str, max_retries: int = 3):\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            data = self.generate_data(user_prompt)\n",
    "            data = data.split(\"\\n\")\n",
    "            valid_data = [item for item in data if len(item.split(\"|||\")) >= 3]\n",
    "            if len(valid_data) > 0:\n",
    "                self.add_valid_data_to_dataset(valid_data, brand, sentiment)\n",
    "                break\n",
    "            else:\n",
    "                retries += 1\n",
    "        if retries == max_retries:\n",
    "            print(f\"Failed to generate valid data for {brand} after {max_retries} attempts.\")\n",
    "        self.report_metrics(brand)\n",
    "\n",
    "    def add_valid_data_to_dataset(self, valid_data: list, brand: str, sentiment: str):\n",
    "        for item in valid_data:\n",
    "            if len(self.dataset) >= self.dataset_size:\n",
    "                break\n",
    "            if item not in self.seen_data:\n",
    "                self.dataset.append(item)\n",
    "                self.seen_data.add(item)\n",
    "                self.brand_counts[brand][sentiment] += 1\n",
    "\n",
    "    def create_prompt(self, brand: str, sentiment: str) -> str:\n",
    "        if brand == \"nobrand\":\n",
    "            return self.create_nobrand_prompt()\n",
    "        else:\n",
    "            return self.create_brand_prompt(brand, sentiment)\n",
    "\n",
    "    def create_nobrand_prompt(self) -> str:\n",
    "        return f\"\"\"\n",
    "            Create some data in the format: \n",
    "            \"tweet\"|||brand\n",
    "            where:\n",
    "                tweet is a Twitter post, \n",
    "                brand is 'nobrand' when no brand is present.\n",
    "            EXAMPLES:\n",
    "                \"Finally finished my first marathon in years and damn, my legs feel like jelly now!\"|||nobrand\n",
    "                \"Today has been such a mad rush, barely got a moment's peace. Is this what Mondays are like?\"|||nobrand\n",
    "                \"Summer really hit us hard today. It's barely spring and I'm already sweating buckets!\"|||nobrand\n",
    "                \"Spent my afternoon curled up with a really good book and a cup of tea. Can't think of a better way to unwind.\"|||nobrand\n",
    "            Create {self.samples_per_brand} data points.\n",
    "            \"\"\"\n",
    "\n",
    "    def create_positive_prompt(self, brand: str) -> str:\n",
    "        if brand == \"nobrand\":\n",
    "            return self.create_nobrand_prompt()\n",
    "        else:\n",
    "            return f\"\"\"\n",
    "                Create some data in the format: \n",
    "                \"tweet\"|||brand|||sentiment \n",
    "                where:\n",
    "                    tweet is a Twitter post, \n",
    "                    brand is {brand}, and\n",
    "                    sentiment is 1 indicating positive sentiment of the tweet.\n",
    "                    EXAMPLES:\n",
    "                        \"My Taco Bell was great guys\"|||Taco Bell|||1\n",
    "                        \"Big thanks to our friends at Hattiesburg Coca-Cola for the banner celebrating R6's run to the national championship. We appreciate your support! Banner is on display in the Cook Union, stop by and see it!\"|||Coca Cola|||1\n",
    "                Create {self.samples_per_brand} data points.\n",
    "                \"\"\"\n",
    "\n",
    "    def create_negative_prompt(self, brand: str) -> str:\n",
    "        if brand == \"nobrand\":\n",
    "            return self.create_nobrand_prompt()\n",
    "        else:\n",
    "            return f\"\"\"\n",
    "                Create some data in the format: \n",
    "                \"tweet\"|||brand|||sentiment \n",
    "                where:\n",
    "                    tweet is a Twitter post, \n",
    "                    brand is {brand}, and\n",
    "                    sentiment is 0 indicating negative sentiment of the tweet.\n",
    "                    EXAMPLES:\n",
    "                        \"It's taken me years, up to this moment, to realize my Gibson guitar sucks ass\"|||Gibson|||0\n",
    "                        \"ppl at netflix are making a lot of really fucking dumb decisions like...way too often. like they got a bad idea assembly line just cranking out mistakes\"|||Netflix|||0\n",
    "                Create {self.samples_per_brand} data points.\n",
    "                \"\"\"\n",
    "\n",
    "    def generate_positive_data(self, brand: str, count: int = None):\n",
    "        while self.brand_counts[brand][\"positive\"] < self.samples_per_brand // 2:\n",
    "            user_prompt = self.create_positive_prompt(brand)\n",
    "            chunks = self.samples_per_brand // 50\n",
    "            for _ in range(chunks):\n",
    "                self.generate_and_add_data(user_prompt, brand, \"positive\")\n",
    "                if self.brand_counts[brand][\"positive\"] >= self.samples_per_brand // 2:\n",
    "                    break\n",
    "            remaining = self.samples_per_brand % 50  # calculate the remaining samples\n",
    "            if remaining > 0 and self.brand_counts[brand][\"positive\"] < self.samples_per_brand // 2:\n",
    "                self.generate_and_add_data(user_prompt, brand, \"positive\", remaining)  # generate the remaining samples\n",
    "            self.save_generated_data()\n",
    "            self.delete_excess_data()  \n",
    "            self.report_metrics(brand)\n",
    "\n",
    "    def generate_negative_data(self, brand: str, count: int = None):\n",
    "        while self.brand_counts[brand][\"negative\"] < self.samples_per_brand // 2:\n",
    "            user_prompt = self.create_negative_prompt(brand)\n",
    "            chunks = self.samples_per_brand // 50\n",
    "            for _ in range(chunks):\n",
    "                self.generate_and_add_data(user_prompt, brand, \"negative\")\n",
    "                if self.brand_counts[brand][\"negative\"] >= self.samples_per_brand // 2:\n",
    "                    break\n",
    "            remaining = self.samples_per_brand % 50  # calculate the remaining samples\n",
    "            if remaining > 0 and self.brand_counts[brand][\"negative\"] < self.samples_per_brand // 2:\n",
    "                self.generate_and_add_data(user_prompt, brand, \"negative\", remaining)  # generate the remaining samples\n",
    "            self.save_generated_data()\n",
    "            self.delete_excess_data()  \n",
    "            self.report_metrics(brand)\n",
    "            \n",
    "    def generate_data(self, user_prompt: str) -> str:\n",
    "        wait_time = 5\n",
    "        retries = 0\n",
    "        while retries < 3:\n",
    "            try:\n",
    "                chat_completion = openai.ChatCompletion.create(\n",
    "                    model=self.model,\n",
    "                    temperature=0.4,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": user_prompt},\n",
    "                    ],\n",
    "                )\n",
    "                return chat_completion.choices[0].message.content\n",
    "            except openai.OpenAIError as e:\n",
    "                if \"Rate limit exceeded\" in str(e) or \"Bad Gateway\" in str(e):\n",
    "                    logging.error(f\"Error: {str(e)}. Retrying in {wait_time} seconds...\")\n",
    "                    print(f\"Error: {str(e)}. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    wait_time *= 2\n",
    "                    retries += 1\n",
    "                else:\n",
    "                    logging.error(f\"Unexpected error: {str(e)}\")\n",
    "                    raise e\n",
    "        logging.error(\"Failed to generate data after 3 retries.\")\n",
    "        raise Exception(\"Failed to generate data after 3 retries.\")\n",
    "\n",
    "    def load_or_initialize_dataset(self):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            self.load_generated_data(self.cache_file)\n",
    "        else:\n",
    "            self.initialize_dataset()\n",
    "            self.save_generated_data() \n",
    "\n",
    "    def initialize_dataset(self):\n",
    "        self.dataset = []\n",
    "        self.seen_data = set()\n",
    "        self.brand_counts = {\n",
    "            brand: {\"positive\": 0, \"negative\": 0} for brand in self.brands\n",
    "        }\n",
    "        self.brand_counts[\"nobrand\"] = {\"positive\": 0, \"negative\": 0}\n",
    "\n",
    "    def save_generated_data(self):\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            pickle.dump(self.dataset, f)\n",
    "\n",
    "    def load_generated_data(self, filename: str):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "        if isinstance(loaded_data, list):\n",
    "            self.dataset = loaded_data\n",
    "            self.update_counts()\n",
    "        else:\n",
    "            print(\n",
    "                \"Loaded data is not in the expected format (list), initializing a new dataset.\"\n",
    "            )\n",
    "            self.initialize_dataset()\n",
    "\n",
    "    def update_counts(self):\n",
    "        for data in self.dataset:\n",
    "            parts = data.split(\"|||\")\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            brand = parts[1].lower() \n",
    "            sentiment = \"positive\" if parts[2] == \"1\" else \"negative\"\n",
    "            self.brand_counts[brand][sentiment] += 1\n",
    "            \n",
    "    def report_metrics(self, last_updated_brand: str = None):\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        brands = []\n",
    "        positive_counts = []\n",
    "        negative_counts = []\n",
    "        status = []\n",
    "\n",
    "        for brand in self.brand_counts.keys():\n",
    "            if brand != \"nobrand\":\n",
    "                brands.append(brand)\n",
    "                positive_counts.append(f\"{self.brand_counts[brand]['positive']}/{self.samples_per_brand//2}\")\n",
    "                negative_counts.append(f\"{self.brand_counts[brand]['negative']}/{self.samples_per_brand//2}\")\n",
    "                status.append(self.get_status(brand)) \n",
    "\n",
    "        df_brands = pd.DataFrame(\n",
    "            {\n",
    "                \"Brands\": brands,\n",
    "                \"Positive Counts\": positive_counts,\n",
    "                \"Negative Counts\": negative_counts,\n",
    "                \"Status\": status,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        df_nobrand = pd.DataFrame(\n",
    "            {\n",
    "                \"Brand\": [\"nobrand\"],\n",
    "                \"Count\": [\n",
    "                    f\"{self.brand_counts['nobrand']['positive'] + self.brand_counts['nobrand']['negative']}/{self.samples_per_brand}\"\n",
    "                ],\n",
    "                \"Status\": [\n",
    "                    Status.BALANCED.name\n",
    "                    if self.brand_counts[\"nobrand\"][\"positive\"]\n",
    "                    + self.brand_counts[\"nobrand\"][\"negative\"]\n",
    "                    == self.samples_per_brand\n",
    "                    else Status.QUEUED.name\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        df_jobs = pd.DataFrame([(job.brand, job.sentiment, job.action.name , job.count) for job in self.job_queue.queue], columns=['Brand', 'Sentiment', 'Action', 'Count'])\n",
    "        \n",
    "        df1_styler = df_brands.style.set_table_attributes(\"style='display:inline; vertical-align:top'\").set_caption('Brands')\n",
    "        df2_styler = df_nobrand.style.set_table_attributes(\"style='display:inline; vertical-align:top'\").set_caption('No Brand')\n",
    "        df3_styler = df_jobs.style.set_table_attributes(\"style='display:inline; vertical-align:top'\").set_caption('Jobs')\n",
    "\n",
    "        display_html(df1_styler._repr_html_() + df2_styler._repr_html_() + df3_styler._repr_html_(), raw=True)\n",
    "\n",
    "        \n",
    "    def get_status(self, brand: str) -> Status:\n",
    "        if brand is None:\n",
    "            return Status.NOT_STARTED.name\n",
    "        elif brand == \"nobrand\":\n",
    "            if (\n",
    "                self.brand_counts[brand][\"positive\"]\n",
    "                + self.brand_counts[brand][\"negative\"]\n",
    "                == self.samples_per_brand\n",
    "            ):\n",
    "                return Status.BALANCED.name\n",
    "            else:\n",
    "                return Status.QUEUED.name\n",
    "        else:\n",
    "            if (\n",
    "                self.brand_counts[brand][\"positive\"]\n",
    "                + self.brand_counts[brand][\"negative\"]\n",
    "                == self.samples_per_brand\n",
    "            ):\n",
    "                return Status.BALANCED.name\n",
    "            elif ( self.brand_counts[brand][\"positive\"] > 0\n",
    "                or self.brand_counts[brand][\"negative\"] > 0\n",
    "            ):\n",
    "                return Status.BALANCING.name\n",
    "            else:\n",
    "                return Status.QUEUED.name\n",
    "\n",
    "    def delete_excess_data(self):\n",
    "        for brand in self.brands + [\"nobrand\"]:\n",
    "            for sentiment in [\"positive\", \"negative\"]:\n",
    "                while self.brand_counts[brand][sentiment] > self.samples_per_brand // 2:\n",
    "                    for i in range(len(self.dataset) - 1, -1, -1):\n",
    "                        parts = self.dataset[i].split(\"|||\")\n",
    "                        if len(parts) < 3:\n",
    "                            continue\n",
    "                        data_brand = parts[1]\n",
    "                        data_sentiment = \"positive\" if parts[2] == \"1\" else \"negative\"\n",
    "                        if data_brand == brand and data_sentiment == sentiment:\n",
    "                            del self.dataset[i]\n",
    "                            self.brand_counts[brand][sentiment] -= 1\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = DataGenerator(BRANDS, samples_per_brand=100)\n",
    "data_generator.calculate_jobs()\n",
    "data_generator.process_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and Integrating Generated Data\n",
    "\n",
    "The raw content generated by the GPT-4 model is processed by splitting it into three parts: 'tweet', 'brand-presence', and 'brand'. This is achieved by separating each line of the raw content on the delimiter '|||'. The processed content is then stored in a structured format as a list of dictionaries.\n",
    "\n",
    "The processed content is converted into a pandas DataFrame and added to the existing `brands_df` DataFrame. This integration step ensures the newly generated data is combined with any current data for further analysis. Any missing values in the 'brand' column of the DataFrame are filled with 'nobrand'. This step ensures data consistency by providing a placeholder value for tweets where no brand is mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_generator\u001b[39m.\u001b[39mload_generated_data(\u001b[39m\"\u001b[39m\u001b[39mcache/generated_data.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Convert the data into a DataFrame\u001b[39;00m\n\u001b[1;32m      5\u001b[0m data \u001b[39m=\u001b[39m [item\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m|||\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data_generator\u001b[39m.\u001b[39mdataset]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_generator.load_generated_data(\"cache/generated_data.pkl\")\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "data = [item.split(\"|||\") for item in data_generator.dataset]\n",
    "brands_df = pd.DataFrame(data, columns=[\"tweet\", \"brand\", \"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"brands_df.shape: {brands_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"brands_df.brand.value_counts(): {brands_df.brand.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"brands_df.tail(6): \\n{brands_df.tail(6)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brand Mention Distribution Visualization\n",
    "\n",
    "We create a visual representation of the distribution of brand mentions within a collection of tweets. We use a bar plot to illustrate the count of tweets mentioning a brand versus those not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_presence = brands_df[\"brand\"].apply(\n",
    "    lambda x: \"No Brand\" if x == \"nobrand\" else \"Brand\"\n",
    ")\n",
    "print(brand_presence.value_counts())\n",
    "\n",
    "counts = brand_presence.value_counts().reset_index()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = sns.barplot(x=\"index\", y=brand_presence.name, data=counts)\n",
    "plt.title(\"Presence of Brand in Tweets\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "for p in ax.patches:\n",
    "    # ADD COUNTS TO BARS\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2.0,\n",
    "        p.get_height(),\n",
    "        \"%d\" % int(p.get_height()),\n",
    "        fontsize=12,\n",
    "        color=\"black\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "# SAVE VISUALIZATION\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"report/brand_count.png\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_counts = brands_df[\"brand\"].value_counts()\n",
    "print(brand_counts)\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.pie(\n",
    "    brand_counts,\n",
    "    labels=brand_counts.index,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=140,\n",
    "    pctdistance=0.85,\n",
    "    labeldistance=1,\n",
    ")\n",
    "plt.axis(\"equal\")\n",
    "plt.title(\"Proportion of Tweets Referencing Each Brand\")\n",
    "\n",
    "# SAVE VISUALIZATION\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"report/proportion_brands_in_tweets.png\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Toolkit (`nltk`) for Text Processing\n",
    "\n",
    "We initialize the Natural Language Toolkit (`nltk`), a Python library for processing human language data. It offers tools for various tasks, including classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "\n",
    "Key parts from `nltk` are downloaded: a tokenizer, a part-of-speech tagger, a named entity chunker, a corpus of words, a multilingual lexical database, and a corpus of stopwords. \n",
    "\n",
    "The tokenizer helps break down the text into sentences or words, the part-of-speech tagger assigns parts of speech to individual words, and the named entity chunker identifies named entities within the text.\n",
    "\n",
    "The corpus of words and the multilingual lexical database can be used for tasks such as spellchecking or language identification.\n",
    "\n",
    "The stopwords corpus lists common words, known as \"stop words\", in several languages, including English. These words are often filtered out during text processing and rarely have significant meaning.\n",
    "\n",
    "We generate a set of English stop words to expedite membership checking. This set will be used later in the text processing pipeline to filter out stop words from the text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUNKT TOKENIZER FOR SENTENCE TOKENIZATION\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# TAGGER FOR PART-OF-SPEECH TAGGING\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "\n",
    "# CHUNKER FOR NAMED ENTITY RECOGNITION\n",
    "nltk.download(\"maxent_ne_chunker\", quiet=True)\n",
    "\n",
    "# CORPUS OF WORDS FOR SPELLCHECKING OR LANGUAGE IDENTIFICATION\n",
    "nltk.download(\"words\", quiet=True)\n",
    "\n",
    "# MULTILINGUAL LEXICAL DATABASE\n",
    "nltk.download(\"omw-1.4\", quiet=True)\n",
    "\n",
    "# DOWNLOAD STOPWORDS CORPUS\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "# CREATE SET OF ENGLISH STOPWORDS FOR FASTER MEMBERSHIP CHECKING\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Tweets for Text Analysis\n",
    "\n",
    "We implement a preprocessing function to clean and standardize the text in tweets. This function removes URLs and @mentions, tokenizes the text into individual words, removes stopwords and punctuation, and applies stemming to reduce words to their root form. This preprocessing function is then applied to each tweet in the dataset. A sample of the original and preprocessed tweets is displayed to visually check the preprocessing results. This step ensures the preprocessing function works as expected and the tweets are ready for further text analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET OPTIONS TO REMOVE URLS AND @MENTIONS\n",
    "preprocessor.set_options(preprocessor.OPT.URL, preprocessor.OPT.MENTION)\n",
    "\n",
    "# INITIALIZE STEMMER\n",
    "STEMMER = SnowballStemmer(\"english\")\n",
    "\n",
    "# CONVERT STOPWORDS AND PUNCTUATION TO SETS\n",
    "STOP_WORDS_SET = set(STOP_WORDS)\n",
    "PUNCTUATION_SET = set(string.punctuation)\n",
    "\n",
    "\n",
    "def preprocess(tweet: str) -> list:\n",
    "    \"\"\"\n",
    "    Function to preprocess a tweet by removing URLs, @mentions, stopwords and punctuation.\n",
    "    It also tokenizes and stems the tweet.\n",
    "\n",
    "    Args:\n",
    "        tweet (str): The tweet to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        list: The preprocessed tweet as a list of stemmed tokens.\n",
    "    \"\"\"\n",
    "    # CLEAN TWEET BY REMOVING URLS AND @MENTIONS\n",
    "    cleaned = preprocessor.clean(tweet)\n",
    "    # TOKENIZE TWEET INTO INDIVIDUAL WORDS\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", cleaned)\n",
    "    # REMOVE STOPWORDS AND PUNCTUATION FROM TOKENS\n",
    "    filtered = [\n",
    "        word\n",
    "        for word in tokens\n",
    "        if word not in STOP_WORDS_SET and word not in PUNCTUATION_SET\n",
    "    ]\n",
    "    # APPLY STEMMING TO FILTERED TOKENS\n",
    "    stemmed = [STEMMER.stem(word) for word in filtered]\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "@measure_time(\"preprocessing training data\")\n",
    "def apply_preprocessing(df):\n",
    "    \"\"\"\n",
    "    Function to apply preprocessing to each tweet in the DataFrame using multithreading.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing the tweets.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with the preprocessed tweets.\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        df[\"processed_text\"] = list(executor.map(preprocess, df.tweet))\n",
    "    return df\n",
    "\n",
    "\n",
    "@measure_time(\"saving preprocessed data\")\n",
    "def save_preprocessed(df, filename):\n",
    "    \"\"\"\n",
    "    Function to save preprocessed DataFrame to a file.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame to be saved.\n",
    "        filename (str): The name of the file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(df, f)\n",
    "\n",
    "\n",
    "@measure_time(\"loading preprocessed data\")\n",
    "def load_preprocessed(filename):\n",
    "    \"\"\"\n",
    "    Function to load preprocessed DataFrame from a file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The loaded DataFrame.\n",
    "    \"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "# LOAD FROM FILE IF EXISTS (DELETE FILE TO RE-PREPROCESS)\n",
    "preprocessed_file = \"cache/preprocessed_train.pkl\"\n",
    "if os.path.exists(preprocessed_file):\n",
    "    tweets_df = load_preprocessed(preprocessed_file)\n",
    "else:\n",
    "    # USE MULTITHREADING TO APPLY PREPROCESSING\n",
    "    tweets_df = apply_preprocessing(tweets_df)\n",
    "    save_preprocessed(tweets_df, preprocessed_file)\n",
    "\n",
    "# SHOW SAMPLE PROCESSED TWEETS\n",
    "tweets_sample = tweets_df[tweets_df.sentiment == 1].head()\n",
    "for index, row in tweets_sample.iterrows():\n",
    "    print(f\"ORIGINAL: {row.tweet}\")\n",
    "    print(f\"PROCESSED: {row.processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW FIRST FEW ROWS OF PREPROCESSED TRAINING DATA\n",
    "print(\"\\n\".join([str(text) for text in tweets_df.processed_text.head()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_time(\"getting word sentiment frequencies\")\n",
    "def get_word_sentiment_frequencies(df):\n",
    "    \"\"\"\n",
    "    Get word-sentiment frequencies from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing the tweets and sentiment labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with word-sentiment pairs as keys and their frequencies as values.\n",
    "    \"\"\"\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(df.sentiment.values.tolist(), df.processed_text):\n",
    "        for word in tweet:\n",
    "            pair = (word, y)\n",
    "            freqs[pair] = freqs.get(pair, 0) + 1\n",
    "\n",
    "    freqs_sorted = dict(sorted(freqs.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    return freqs_sorted\n",
    "\n",
    "\n",
    "freqs_sorted = get_word_sentiment_frequencies(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NUMBER OF UNIQUE WORD-SENTIMENT PAIRS: {len(freqs_sorted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_time(\"printing sample preprocessed text\")\n",
    "def print_around_average(freqs_sorted: Dict[Tuple[Any, int], int]) -> None:\n",
    "    \"\"\"\n",
    "    Print the first 5 items, 5 items around the average frequency, and last 5 items in a sorted dictionary.\n",
    "\n",
    "    Args:\n",
    "        freqs_sorted (dict): The sorted dictionary.\n",
    "    \"\"\"\n",
    "    # PRINT FIRST 5 ITEMS IN THE SORTED DICTIONARY\n",
    "    print(\"\\n\".join(f\"{k}: {v}\" for k, v in itertools.islice(freqs_sorted.items(), 5)))\n",
    "\n",
    "    # PRINT 5 ITEMS AROUND THE AVERAGE FREQUENCY\n",
    "    # Calculate the average frequency\n",
    "    avg_freq = sum(freqs_sorted.values()) / len(freqs_sorted)\n",
    "\n",
    "    # Find the index of the first item with a frequency just below the average\n",
    "    below_avg_index = next(\n",
    "        i for i, v in enumerate(freqs_sorted.values()) if v < avg_freq\n",
    "    )\n",
    "\n",
    "    # Get 5 items around the average frequency\n",
    "    start_index = below_avg_index - 2  # 2 items before the average\n",
    "    end_index = below_avg_index + 3  # 2 items after the average\n",
    "\n",
    "    # Convert dictionary items to a list and slice the 5 items around the average\n",
    "    around_avg_items = list(freqs_sorted.items())[start_index:end_index]\n",
    "    print(\"\\n\".join(f\"{k}: {v}\" for k, v in around_avg_items))\n",
    "\n",
    "    # PRINT LAST 5 ITEMS IN THE SORTED DICTIONARY\n",
    "    print(\"\\n\".join(f\"{k}: {v}\" for k, v in list(freqs_sorted.items())[-5:]))\n",
    "\n",
    "\n",
    "print_around_average(freqs_sorted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Sentiment Distribution for Frequently Occurring Word Stems\n",
    "\n",
    "We create a scatter plot to analyze our dataset's 50 most often occurring word stems. Each point on the plot corresponds to a word stem, positioned according to its occurrence in positive and negative sentiments.\n",
    "\n",
    "Handling large frequency counts can lead to complexity in graphical representation. To tackle this, we apply a logarithmic transformation to the data. This technique simplifies the visualization of large numbers, making the information easier to understand. Each point on the graph is labeled with the word stem it represents. Additionally, a line is drawn on the plot to represent an equal frequency of positive and negative sentiments. This element aids in quickly determining whether a word stem is primarily associated with positive or negative sentiments in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_time(\"generating sentiment counts plot\")\n",
    "def generate_sentiment_counts_plot(freqs_sorted, n=50):\n",
    "    # GET UNIQUE LIST OF WORDS FROM KEYS IN FREQS_SORTED\n",
    "    word_list = list(set(word for word, sentiment in freqs_sorted.keys()))\n",
    "\n",
    "    # BUILD LIST OF WORDS WITH POSITIVE AND NEGATIVE SENTIMENT COUNTS\n",
    "    data = [\n",
    "        [word, freqs_sorted.get((word, 1), 0), freqs_sorted.get((word, 0), 0)]\n",
    "        for word in word_list\n",
    "        # Remove i since it is an outlier\n",
    "        if word != \"i\"\n",
    "    ]\n",
    "\n",
    "    # SORT DATA BY THE SUM OF POSITIVE AND NEGATIVE COUNTS AND LIMIT TO TOP 'n' MOST FREQUENT WORDS\n",
    "    data = sorted(data, key=lambda x: x[1] + x[2], reverse=True)[:n]\n",
    "\n",
    "    # PREPARE X AND Y FOR PLOT, APPLY LOG TRANSFORMATION FOR LARGE COUNTS\n",
    "    x = np.log([x[1] + 1 for x in data])\n",
    "    y = np.log([x[2] + 1 for x in data])\n",
    "\n",
    "    # CREATE SCATTER PLOT FOR LOG-TRANSFORMED COUNTS\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.scatterplot(x=x, y=y)\n",
    "    plt.title(f\"Positive and Negative Counts of {n} Most Frequent Word Stems\")\n",
    "    plt.xlabel(\"Log Positive count\")\n",
    "    plt.ylabel(\"Log Negative count\")\n",
    "\n",
    "    # ADD ANNOTATIONS TO EACH POINT IN SCATTER PLOT, ADJUST TO AVOID OVERLAP\n",
    "    texts = []\n",
    "    for i in range(len(data)):\n",
    "        texts.append(plt.text(x[i], y[i], data[i][0], fontsize=12))\n",
    "\n",
    "    adjust_text(texts)\n",
    "\n",
    "    # ADD RED LINE FROM MINIMUM TO MAXIMUM VALUE ON BOTH AXES\n",
    "    min_val = min(np.min(x), np.min(y))\n",
    "    max_val = max(np.max(x), np.max(y))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color=\"red\")\n",
    "\n",
    "    # SAVE VISUALIZATION\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"report/positive_and_negative_counts.png\", dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function\n",
    "generate_sentiment_counts_plot(freqs_sorted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Evaluate Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights & Biases (`wandb`) Setup\n",
    "\n",
    "We use Weights & Biases (wandb) for experiment tracking. This MLOps tool allows us to compare metrics from different runs and visualize model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brand Classifer\n",
    "\n",
    "The `BrandClassifier` class is a comprehensive tool created to identify brand mentions in tweets. It operates with a specified machine-learning model and a vectorizer responsible for transforming text data into a numerical format that the model can process. Once configured, the `BrandClassifier` can train the model with given data, predict based on new data inputs, and evaluate the accuracy of the model's performance. The class structures data into training, testing, and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrandClassifier:\n",
    "    \"\"\"A generic brand classifier.\"\"\"\n",
    "\n",
    "    @measure_time(\"initializing classifier\")\n",
    "    def __init__(\n",
    "        self, classifier, vectorizer: CountVectorizer = CountVectorizer()\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the classifier with a vectorizer and classifier.\n",
    "\n",
    "        Args:\n",
    "            classifier: The classifier to use.\n",
    "            vectorizer: The vectorizer to use. Defaults to CountVectorizer().\n",
    "        \"\"\"\n",
    "        self.model = Pipeline([(\"vectorizer\", vectorizer), (\"classifier\", classifier)])\n",
    "        datetime_stamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        classifier_name = self.model.named_steps[\"classifier\"].__class__.__name__\n",
    "        wandb.init(\n",
    "            project=\"brand-sentiment-analysis\",\n",
    "            name=f\"{datetime_stamp}-{classifier_name}\",\n",
    "        )\n",
    "\n",
    "    @measure_time(\"training classifier\")\n",
    "    def fit(self, X_train: List[str], y_train: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Fits the model to the training data.\n",
    "\n",
    "        Args:\n",
    "            X_train: The training data.\n",
    "            y_train: The training labels.\n",
    "        \"\"\"\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    @measure_time(\"predicting sentiment\")\n",
    "    def predict(self, X: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Predicts the labels for the given data.\n",
    "\n",
    "        Args:\n",
    "            X: The data to predict labels for.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The predicted labels.\n",
    "        \"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    @measure_time(\"evaluating classifier\")\n",
    "    def evaluate(self, X: List[str], y: List[str]) -> Tuple[float, float, float, float]:\n",
    "        \"\"\"\n",
    "        Evaluates the model on the given data and returns the accuracy, precision, recall, and F1 score.\n",
    "\n",
    "        Args:\n",
    "            X: The data to evaluate the model on.\n",
    "            y: The true labels for the data.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float, float, float]: The accuracy, precision, recall, and F1 score.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        precision = precision_score(y, y_pred)\n",
    "        recall = recall_score(y, y_pred)\n",
    "        f1 = f1_score(y, y_pred)\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    @measure_time(\"printing classifier results\")\n",
    "    def print_scores(\n",
    "        self, scores: Tuple[float, float, float, float], data_name: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Prints the accuracy, precision, recall, and F1 score.\n",
    "\n",
    "        Args:\n",
    "            scores: The scores to print.\n",
    "            data_name: The name of the data the scores are for.\n",
    "        \"\"\"\n",
    "        accuracy, precision, recall, f1 = scores\n",
    "        print(f\"{data_name} Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"{data_name} Precision: {precision:.4f}\")\n",
    "        print(f\"{data_name} Recall: {recall:.4f}\")\n",
    "        print(f\"{data_name} F1: {f1:.4f}\")\n",
    "\n",
    "    @measure_time(\"finishing run\")\n",
    "    def finish(self, scores: Tuple[float, float, float, float]) -> None:\n",
    "        \"\"\"\n",
    "        Finishes the wandb run and saves the trained model as an artifact.\n",
    "\n",
    "        Args:\n",
    "            scores: The scores of the model.\n",
    "        \"\"\"\n",
    "        # INITIALIZE NEW ARTIFACT\n",
    "        classifier_name = self.model.named_steps[\"classifier\"].__class__.__name__\n",
    "        accuracy, precision, recall, f1 = scores\n",
    "\n",
    "        artifact = wandb.Artifact(\n",
    "            classifier_name,\n",
    "            type=\"model\",\n",
    "            description=\"Trained model for brand sentiment analysis\",\n",
    "            metadata={\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # ADD MODEL TO ARTIFACT\n",
    "        joblib.dump(self.model, \"model.pkl\")\n",
    "        artifact.add_file(\"model.pkl\", name=\"classifier_name\")\n",
    "\n",
    "        # LOG ARTIFACT TO wandb\n",
    "        wandb.log_artifact(artifact)\n",
    "\n",
    "        # DELETE TEMPORARY ARTIFACT FROM FILESYSTEM\n",
    "        os.remove(\"model.pkl\")\n",
    "\n",
    "        # FINISH RUN\n",
    "        wandb.finish()\n",
    "\n",
    "    @measure_time(\"splitting data\")\n",
    "    def split_data(\n",
    "        self, X: List[str], y: List[str]\n",
    "    ) -> Tuple[List[str], List[str], List[str], List[str], List[str], List[str]]:\n",
    "        \"\"\"\n",
    "        Splits the data into training, test, validation sets.\n",
    "\n",
    "        Args:\n",
    "            X: The data to split.\n",
    "            y: The labels to split.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[str], List[str], List[str], List[str], List[str], List[str]]: The train, test, validate data and labels.\n",
    "        \"\"\"\n",
    "        # SPLIT DATA INTO TRAINING SET AND TEMP SET WITH 80/20 SPLIT\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # SPLIT TEMP SET INTO VALIDATION SET AND TEST SET WITH 50/50 SPLIT\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_temp, y_temp, test_size=0.5, random_state=42\n",
    "        )\n",
    "\n",
    "        return X_train, y_train, X_test, y_test, X_val, y_val\n",
    "\n",
    "    @measure_time(\"running classifier\")\n",
    "    def run(self, X: List[str], y: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Runs the entire process from training to testing.\n",
    "\n",
    "        Args:\n",
    "            X: The data to use.\n",
    "            y: The labels to use.\n",
    "        \"\"\"\n",
    "        # SPLIT DATA\n",
    "        X_train, y_train, X_test, y_test, X_val, y_val = self.split_data(X, y)\n",
    "\n",
    "        # FIT MODEL\n",
    "        self.fit(X_train, y_train)\n",
    "\n",
    "        # EVALUATE MODEL ON VALIDATION SET\n",
    "        val_scores = self.evaluate(X_val, y_val)\n",
    "        self.print_scores(val_scores, \"Validation\")\n",
    "\n",
    "        # EVALUATE MODEL ON TEST SET\n",
    "        test_scores = self.evaluate(X_test, y_test)\n",
    "        self.print_scores(test_scores, \"Test\")\n",
    "\n",
    "        # FINISH RUN\n",
    "        self.finish(test_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes (`MultinomialNB`)\n",
    "\n",
    "The Multinomial Naive Bayes classifier is a widely recognized and used tool in text classification. This classifier is based on Bayes' theorem, a principle in probability theory and statistics that describes the probability of an event based on prior knowledge of conditions that might be related to the event. The Multinomial Naive Bayes classifier assumes that all features, such as words in a tweet, are independent of each other. This is often called the 'naive' assumption, hence the name Naive Bayes.\n",
    "\n",
    "This classifier is effective when dealing with features that are discrete counts. In text classification, this could be the number of times a specific word appears in a document or a tweet. The Multinomial Naive Bayes classifier requires these counts to be integers, aligning perfectly with word counts in text data. This makes the Multinomial Naive Bayes classifier an ideal choice for our text classification needs, as it can effectively handle and make predictions based on the vectorized text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BrandClassifier(MultinomialNB()).run(brands_df.tweet, brands_df.brand_presence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brand classification using Multinomial Naive Bayes using brand (name) as the target\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(brands_df.tweet)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(brands_df.brand)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)\n",
    "model = MultinomialNB()\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "predictions = model.predict(xtest)\n",
    "predicted_brand_names = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "accuracy = accuracy_score(ytest, predictions)\n",
    "precision_micro = precision_score(ytest, predictions, average=\"micro\")\n",
    "precision_macro = precision_score(ytest, predictions, average=\"macro\")\n",
    "precision_weighted = precision_score(ytest, predictions, average=\"weighted\")\n",
    "# recall = recall_score(ytest, predictions)\n",
    "# f1 = f1_score(ytest, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (micro average): {precision_micro:.4f}\")\n",
    "print(f\"Precision (macro average): {precision_macro:.4f}\")\n",
    "print(f\"Precision (weighted average): {precision_weighted:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1: {f1:.4f}\")\n",
    "\n",
    "class_labels = label_encoder.inverse_transform(range(len(label_encoder.classes_)))\n",
    "print(class_labels)\n",
    "cm = confusion_matrix(ytest, predictions)\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    cm, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Quick test\n",
    "new_tweets = [\n",
    "    \"apple is the best company\",\n",
    "    \"I love apples\",\n",
    "    \"Apples are the worst\",\n",
    "    \"I like apple?\",\n",
    "]\n",
    "\n",
    "new_tweets_vectorized = vectorizer.transform(new_tweets)\n",
    "\n",
    "# Predict the class using the trained model\n",
    "test_prediction = model.predict(new_tweets_vectorized)\n",
    "\n",
    "# If needed, inverse transform the predicted label to get the brand name\n",
    "new_predicted_brand_name = label_encoder.inverse_transform(test_prediction)\n",
    "\n",
    "print(new_predicted_brand_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classifier (`LinearSVC`)\n",
    "\n",
    "The Linear Support Vector Classifier is a Support Vector Machine (SVM) often used for text classification tasks. SVMs are supervised learning methods for classification, regression, and outliers detection. The LinearSVC works by identifying the best boundary or 'hyperplane' that separates different data classes. This is achieved by maximizing the margin between the classes in the training data.\n",
    "\n",
    "One of the strengths of LinearSVC is its ability to handle high-dimensional data. In text classification, once the text data is vectorized, it can result in a high-dimensional feature space, where each dimension corresponds to a unique word in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BrandClassifier(LinearSVC()).run(brands_df.tweet, brands_df.brand_presence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical model used to predict the probability of a specific class or event. Despite its name, Logistic Regression is used for classification tasks, not regression tasks. In text classification, it's used to predict the likelihood of a specific word belonging to a particular category or class, such as whether a tweet mentions a brand.\n",
    "\n",
    "Logistic Regression is known for its simplicity and efficiency. It uses a logistic function to model a binary dependent variable, making it a suitable choice for tasks where the output can be one of two possible outcomes. In our case, this could be whether a tweet mentions a brand or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BrandClassifier(LogisticRegression()).run(brands_df.tweet, brands_df.brand_presence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT METRICS\n",
    "metrics.print_metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classifier ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
