{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Project"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<https://www.analyticsvidhya.com/blog/2021/12/sentiment-analysis-on-tweets-with-lstm-for-beginners/>\n",
                "\n",
                "<https://www.kaggle.com/code/stoicstatic/twitter-sentiment-analysis-for-beginners>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Global Variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from enum import Enum\n",
                "from typing import List, Tuple, Union\n",
                "\n",
                "\n",
                "class Tex(Enum):\n",
                "    SECTION = 1\n",
                "    SUBSECTION = 2\n",
                "    SUBSUBSECTION = 3\n",
                "    SLIDE = 4\n",
                "    TEXT = 5\n",
                "    IMAGE = 6\n",
                "\n",
                "\n",
                "# HOLD CONTENT FOR GENERATING REPORT\n",
                "class Tex(Enum):\n",
                "    SECTION = 1\n",
                "    SUBSECTION = 2\n",
                "    SUBSUBSECTION = 3\n",
                "    SLIDE = 4\n",
                "    TEXT = 5\n",
                "    IMAGE = 6\n",
                "\n",
                "\n",
                "class ContentList(list):\n",
                "    def __init__(self, *args):\n",
                "        super().__init__(*args)\n",
                "        self.index = 0\n",
                "\n",
                "    def append(self, tex: Union[Tex, str], header: str, content: str):\n",
                "        self.append((self.index, tex, header, content))\n",
                "        self.index += 1\n",
                "\n",
                "\n",
                "content: ContentList = ContentList()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Git LFS\n",
                "\n",
                "Pull the datasets from Git LFS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!git lfs pull -I \"datasets/training.1600000.processed.noemoticon.csv\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import, Initialize, & Configure"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Environment Variables\n",
                "\n",
                "`python-dotenv` is used to load environment variables from the `.env` file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import dotenv\n",
                "\n",
                "dotenv.load_dotenv()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### numpy\n",
                "\n",
                "`numpy` is used for numerical processing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Seaborn\n",
                "\n",
                "`seaborn` is used to set the default style for plots."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import seaborn as sns"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Matplotlib\n",
                "\n",
                "`matplotlib` is used to plot the data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pandas\n",
                "\n",
                "`pandas` is a Python library for data manipulation and analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "pd.set_option(\"display.max_colwidth\", 20)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### NLTK"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "import string\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.tokenize import TweetTokenizer\n",
                "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
                "\n",
                "\n",
                "nltk.download('punkt')\n",
                "nltk.download('averaged_perceptron_tagger')\n",
                "nltk.download('maxent_ne_chunker')\n",
                "nltk.download('words')\n",
                "nltk.download('omw-1.4')\n",
                "nltk.download('stopwords')\n",
                "stop_words = set(stopwords.words('english'))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tweet-Preprocessor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install tweet-preprocessor\n",
                "\n",
                "import preprocessor as p"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Weights & Biases\n",
                "\n",
                "[Weights & Biases](https://www.wandb.com/) is used for tracking and visualizing experiments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import wandb\n",
                "\n",
                "wandb.finish()\n",
                "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Should we separate brands by category? e.g. clothing, food, electronics\n",
                "# Define brands of interest\n",
                "brands = ['google', 'facebook', 'microsoft', 'amazon', 'apple', 'walmart', 'nike', 'target', 'starbucks', 'mcdonalds', 'netflix', 'disney']"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "ContentList.append() missing 2 required positional arguments: 'header' and 'content'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## Exploratory Data Analysis (EDA)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m content\u001b[39m.\u001b[39;49mappend((Tex\u001b[39m.\u001b[39;49mSECTION, \u001b[39m\"\u001b[39;49m\u001b[39mExploratory Data Analysis\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
                        "\u001b[0;31mTypeError\u001b[0m: ContentList.append() missing 2 required positional arguments: 'header' and 'content'"
                    ]
                }
            ],
            "source": [
                "## Exploratory Data Analysis (EDA)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Sentiment140 Dataset\n",
                "\n",
                "<http://help.sentiment140.com/for-students>\n",
                "\n",
                "Sentiment140 is a dataset containing 1.6 million tweets with sentiment labels. \n",
                "\n",
                "It allows you to discover the sentiment of a brand, product, or topic on Twitter. \n",
                "\n",
                "The data is a CSV with emoticons removed. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_train = pd.read_csv(\n",
                "    SENTIMENT140_TRAIN,\n",
                "    encoding=\"ISO-8859-1\",\n",
                "    names=SENTIMENT140_FIELDS,\n",
                "    header=None,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>target</th>\n",
                            "      <th>ids</th>\n",
                            "      <th>date</th>\n",
                            "      <th>flag</th>\n",
                            "      <th>user</th>\n",
                            "      <th>text</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0</td>\n",
                            "      <td>1467810369</td>\n",
                            "      <td>Mon Apr 06 22:19...</td>\n",
                            "      <td>NO_QUERY</td>\n",
                            "      <td>_TheSpecialOne_</td>\n",
                            "      <td>@switchfoot http...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>0</td>\n",
                            "      <td>1467810672</td>\n",
                            "      <td>Mon Apr 06 22:19...</td>\n",
                            "      <td>NO_QUERY</td>\n",
                            "      <td>scotthamilton</td>\n",
                            "      <td>is upset that he...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>0</td>\n",
                            "      <td>1467810917</td>\n",
                            "      <td>Mon Apr 06 22:19...</td>\n",
                            "      <td>NO_QUERY</td>\n",
                            "      <td>mattycus</td>\n",
                            "      <td>@Kenichan I dive...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>0</td>\n",
                            "      <td>1467811184</td>\n",
                            "      <td>Mon Apr 06 22:19...</td>\n",
                            "      <td>NO_QUERY</td>\n",
                            "      <td>ElleCTF</td>\n",
                            "      <td>my whole body fe...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>0</td>\n",
                            "      <td>1467811193</td>\n",
                            "      <td>Mon Apr 06 22:19...</td>\n",
                            "      <td>NO_QUERY</td>\n",
                            "      <td>Karoli</td>\n",
                            "      <td>@nationwideclass...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   target         ids                 date      flag             user  \\\n",
                            "0       0  1467810369  Mon Apr 06 22:19...  NO_QUERY  _TheSpecialOne_   \n",
                            "1       0  1467810672  Mon Apr 06 22:19...  NO_QUERY    scotthamilton   \n",
                            "2       0  1467810917  Mon Apr 06 22:19...  NO_QUERY         mattycus   \n",
                            "3       0  1467811184  Mon Apr 06 22:19...  NO_QUERY          ElleCTF   \n",
                            "4       0  1467811193  Mon Apr 06 22:19...  NO_QUERY           Karoli   \n",
                            "\n",
                            "                  text  \n",
                            "0  @switchfoot http...  \n",
                            "1  is upset that he...  \n",
                            "2  @Kenichan I dive...  \n",
                            "3  my whole body fe...  \n",
                            "4  @nationwideclass...  "
                        ]
                    },
                    "execution_count": 88,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df_train.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The dataset has 6 fields:\n",
                "\n",
                "1. `target` — the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
                "1. `id` — the id of the tweet (2087)\n",
                "1. `date` — the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
                "1. `flag` — the query (lyx). If there is no query, then this value is NO_QUERY.\n",
                "1. `user` — the user that tweeted (robotickilldozr)\n",
                "1. `text` — the text of the tweet (Lyx is cool)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"df_train.shape: {df_train.shape}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "outputs": [],
            "source": [
                "raw = f\"\"\"We show that the dataset is balanced by counting the number of positive and negative tweets. Balance is important because it means that the model will be trained on an equal number of positive and negative tweets. If the dataset was imbalanced, then the model would be trained on more of one class than the other. This would result in a model that is biased towards the class with more samples.\n",
                "\"\"\"\n",
                "\n",
                "content.append((Tex.SUBSECTION, f\"Class Balance\", raw))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SHOW SENTIMENT DISTRIBUTION IN TRAINING SET\n",
                "distribution = df_train[\"target\"].value_counts()\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.barplot(x=distribution.index, y=distribution.values)\n",
                "plt.title(\"Distribution of Sentiment in Sentiment140 Training Set\")\n",
                "plt.xlabel(\"Sentiment\")\n",
                "plt.ylabel(\"Count\")\n",
                "plt.xticks([0, 1], [\"Negative\", \"Positive\"])\n",
                "\n",
                "# ADD COUNTS ABOVE THE BARS\n",
                "for i, v in enumerate(distribution.values):\n",
                "    plt.text(i, v, str(v), ha=\"center\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Word Cloud"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_wordcloud(\n",
                "    df: pd.DataFrame, column: str, filename: str, directory: str = \"wordclouds\"\n",
                ") -> None:\n",
                "    \"\"\"\n",
                "    Generate a wordcloud from a column in a dataframe.\n",
                "\n",
                "    Args:\n",
                "        filename (str): Filename to save the wordcloud as.\n",
                "        df (pd.DataFrame): Dataframe.\n",
                "        column (str): Column name.\n",
                "\n",
                "    \"\"\"\n",
                "    from wordcloud import WordCloud\n",
                "\n",
                "    wordcloud = WordCloud(\n",
                "        width=1600,\n",
                "        height=800,\n",
                "        background_color=\"white\",\n",
                "        min_font_size=10,\n",
                "        max_words=1000,\n",
                "        collocations=False,\n",
                "    )\n",
                "\n",
                "    wordcloud.generate(\" \".join(df[column].tolist()))\n",
                "\n",
                "    plt.figure(figsize=(8, 8))\n",
                "    plt.imshow(wordcloud)\n",
                "    plt.axis(\"off\")\n",
                "\n",
                "    if not os.path.exists(directory):\n",
                "        os.makedirs(directory, exist_ok=True)\n",
                "\n",
                "    output_path = os.path.join(directory, filename)\n",
                "    plt.savefig(output_path, format=\"png\")\n",
                "    plt.show()\n",
                "    plt.close()\n",
                "\n",
                "\n",
                "generate_wordcloud(df_train, \"text\", \"wordcloud_train.png\", directory=\"report\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocessing"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Preprocessing is traditionally an important step for Natural Language Processing (NLP) tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.\n",
                "\n",
                "Preprocessing steps include:\n",
                "\n",
                "* Lower Casing: Each text is converted to lowercase.\n",
                "* Replacing URLs: Links starting with \"http\" or \"https\" or \"www\" are replaced by \"URL\".\n",
                "* Replacing Emojis: Replace emojis by using a pre-defined dictionary containing emojis along with their meaning. (eg: \":)\" to \"EMOJIsmile\")\n",
                "* Replacing Usernames: Replace @Usernames with word \"USER\". (eg: \"@Kaggle\" to \"USER\")\n",
                "* Removing Non-Alphabets: Replacing characters except Digits and Alphabets with a space.\n",
                "* Removing Consecutive letters: 3 or more consecutive letters are replaced by 2 letters. (eg: \"Heyyyy\" to \"Heyy\")\n",
                "* Removing Short Words: Words with length less than 2 are removed.\n",
                "* Removing Stopwords: Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. (eg: \"the\", \"he\", \"have\")\n",
                "* Lemmatizing: Lemmatization is the process of converting a word to its base form. (e.g: \"Great\" to \"Good\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 89,
            "metadata": {},
            "outputs": [],
            "source": [
                "# REMOVE UNNECESSARY COLUMNS\n",
                "df_train = df_train[[\"text\", \"target\"]]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Re-map sentiment labels to be `0 = negative` and `1 = positive` instead of of `4 = positive`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 90,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_train[\"target\"] = df_train[\"target\"].replace(4, 1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 91,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0          0\n",
                            "1          0\n",
                            "2          0\n",
                            "3          0\n",
                            "4          0\n",
                            "          ..\n",
                            "1599995    1\n",
                            "1599996    1\n",
                            "1599997    1\n",
                            "1599998    1\n",
                            "1599999    1\n",
                            "Name: target, Length: 1600000, dtype: int64"
                        ]
                    },
                    "execution_count": 91,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# CHECK\n",
                "df_train[\"target\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 92,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "TWEETS SAMPLE:\n",
                        "I LOVE @Health4UandPets u guys r the best!! \n",
                        "im meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!\n",
                        "@DaRealSunisaKim Thanks for the Twitter add, Sunisa! I got to meet you once at a HIN show here in the DC area and you were a sweetheart. \n",
                        "Being sick can be really cheap when it hurts too much to eat real food  Plus, your friends make you soup\n",
                        "@LovesBrooklyn2 he has that effect on everyone \n",
                        "AFTER CLEANING URLS AND @ MENTIONS\n",
                        "I LOVE u guys r the best!!\n",
                        "im meeting up with one of my besties tonight! Cant wait!! - GIRL TALK!!\n",
                        "Thanks for the Twitter add, Sunisa! I got to meet you once at a HIN show here in the DC area and you were a sweetheart.\n",
                        "Being sick can be really cheap when it hurts too much to eat real food Plus, your friends make you soup\n",
                        "he has that effect on everyone\n",
                        "AFTER TOKENIZATION:\n",
                        "['I', 'LOVE', 'u', 'guys', 'r', 'the', 'best', '!', '!']\n",
                        "['im', 'meeting', 'up', 'with', 'one', 'of', 'my', 'besties', 'tonight', '!', 'Cant', 'wait', '!', '!', '-', 'GIRL', 'TALK', '!', '!']\n",
                        "['Thanks', 'for', 'the', 'Twitter', 'add', ',', 'Sunisa', '!', 'I', 'got', 'to', 'meet', 'you', 'once', 'at', 'a', 'HIN', 'show', 'here', 'in', 'the', 'DC', 'area', 'and', 'you', 'were', 'a', 'sweetheart', '.']\n",
                        "['Being', 'sick', 'can', 'be', 'really', 'cheap', 'when', 'it', 'hurts', 'too', 'much', 'to', 'eat', 'real', 'food', 'Plus', ',', 'your', 'friends', 'make', 'you', 'soup']\n",
                        "['he', 'has', 'that', 'effect', 'on', 'everyone']\n",
                        "AFTER REMOVING STOP WORDS AND PUNCTUATION\n",
                        "['I', 'LOVE', 'u', 'guys', 'r', 'best']\n",
                        "['im', 'meeting', 'one', 'besties', 'tonight', 'Cant', 'wait', 'GIRL', 'TALK']\n",
                        "['Thanks', 'Twitter', 'add', 'Sunisa', 'I', 'got', 'meet', 'HIN', 'show', 'DC', 'area', 'sweetheart']\n",
                        "['Being', 'sick', 'really', 'cheap', 'hurts', 'much', 'eat', 'real', 'food', 'Plus', 'friends', 'make', 'soup']\n",
                        "['effect', 'everyone']\n",
                        "AFTER STEMMING:\n",
                        "['i', 'love', 'u', 'guy', 'r', 'best']\n",
                        "['im', 'meet', 'one', 'besti', 'tonight', 'cant', 'wait', 'girl', 'talk']\n",
                        "['thank', 'twitter', 'add', 'sunisa', 'i', 'got', 'meet', 'hin', 'show', 'dc', 'area', 'sweetheart']\n",
                        "['be', 'sick', 'realli', 'cheap', 'hurt', 'much', 'eat', 'real', 'food', 'plu', 'friend', 'make', 'soup']\n",
                        "['effect', 'everyon']\n",
                        "['awww', \"that'\", 'bummer', 'you', 'shoulda', 'got', 'david', 'carr', 'third', 'day', ';d']\n",
                        "['upset', \"can't\", 'updat', 'facebook', 'text', '...', 'might', 'cri', 'result', 'school', 'today', 'also', 'blah']\n",
                        "['i', 'dive', 'mani', 'time', 'ball', 'manag', 'save', '50', 'the', 'rest', 'go', 'bound']\n",
                        "['whole', 'bodi', 'feel', 'itchi', 'like', 'fire']\n",
                        "['behav', \"i'm\", 'mad', 'i', \"can't\", 'see']\n"
                    ]
                }
            ],
            "source": [
                "# Select sample of tweets to demonstrate preprocessing steps\n",
                "tweets_sample = df_train[df_train[\"target\"] == 1].head()\n",
                "\n",
                "def pretty_print_column(text):\n",
                "    print(*text.tolist(), sep='\\n')\n",
                "\n",
                "print(\"TWEETS SAMPLE:\")\n",
                "pretty_print_column(tweets_sample[\"text\"])\n",
                "\n",
                "# Initialize twitter-specific tokenizer\n",
                "tweet_tokenizer = TweetTokenizer()\n",
                "\n",
                "# Initialize stemmer\n",
                "stemmer = PorterStemmer()\n",
                "\n",
                "# Set options for tweet preprocessor to remove URLs and @ mentions\n",
                "p.set_options(p.OPT.URL, p.OPT.MENTION)\n",
                "\n",
                "print(\"AFTER CLEANING URLS AND @ MENTIONS\")\n",
                "tweets_sample.text = tweets_sample.text.apply(lambda x: p.clean(x))\n",
                "pretty_print_column(tweets_sample.text)\n",
                "\n",
                "def tokenize_tweets(tweet):\n",
                "    # Use twitter-specific tokenizer to tokenize tweets\n",
                "    tokens = tweet_tokenizer.tokenize(tweet)\n",
                "    return tokens\n",
                "\n",
                "print(\"AFTER TOKENIZATION:\")\n",
                "tweets_sample.text = tweets_sample.text.apply(tokenize_tweets)\n",
                "pretty_print_column(tweets_sample.text)\n",
                "\n",
                "def remove_stopwords_twitter_specific(tokens):\n",
                "    # Filter stop words and punctuation from tweet\n",
                "    filtered_tweet = [word for word in tokens if not word in stop_words and not word in string.punctuation]\n",
                "    return filtered_tweet\n",
                "\n",
                "print(\"AFTER REMOVING STOP WORDS AND PUNCTUATION\")\n",
                "tweets_sample.text = tweets_sample.text.apply(remove_stopwords_twitter_specific)\n",
                "pretty_print_column(tweets_sample.text)\n",
                "\n",
                "# NOTE: Use either stemming or lemmatization, not both (stemming is probably better for a dataset for this size)\n",
                "def apply_stemming(tweet_tokens):\n",
                "    stemmed = [stemmer.stem(word) for word in tweet_tokens]\n",
                "    return stemmed\n",
                "\n",
                "print(\"AFTER STEMMING:\")\n",
                "tweets_sample.text = tweets_sample.text.apply(apply_stemming)\n",
                "pretty_print_column(tweets_sample.text)\n",
                "\n",
                "\n",
                "# Combine into one method to be used on full data\n",
                "def preprocess(tweet):\n",
                "    cleaned = p.clean(tweet)\n",
                "    tokens = tokenize_tweets(cleaned)\n",
                "    filtered = remove_stopwords_twitter_specific(tokens)\n",
                "    stemmed = apply_stemming(filtered)\n",
                "    return stemmed\n",
                "\n",
                "df_train[\"text\"] = df_train[\"text\"].apply(preprocess)\n",
                "pretty_print_column(df_train[\"text\"].head())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Stopwords\n",
                "\n",
                "Stopwords like \"the\", \"a\", and \"is\" are so common that they are not useful for training the model.\n",
                "\n",
                "The `nltk` library is used to remove stopwords."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import re\n",
                "# import nltk\n",
                "\n",
                "# nltk.download(\"stopwords\")\n",
                "# from nltk.corpus import stopwords\n",
                "# from nltk.stem.porter import PorterStemmer\n",
                "\n",
                "# corpus = []\n",
                "# for i in range(0, int(df_train.shape[0])):\n",
                "#     text = re.sub(\"[^a-zA-z]\", \" \", df_train[\"text\"][i])\n",
                "#     text = text.lower()\n",
                "#     text = text.split()\n",
                "#     ps = PorterStemmer()\n",
                "#     all_stopwords = stopwords.words(\"english\")\n",
                "#     all_stopwords.remove(\"not\")\n",
                "#     text = [ps.stem(word) for word in text if not word in set(all_stopwords)]\n",
                "#     text = \" \".join(text)\n",
                "#     corpus.append(text)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Stemming is used to reduce words to their root form. We do this because many words have the same meaning but are written differently.\n",
                "For example, \"run\", \"runs\", and \"running\" all have the same meaning. This will reduce the number of unique words in the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: STEMMING/LEMMATIZATION"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Split the dataset into train and test sets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    df_train[\"text\"], df_train[\"target\"], test_size=0.2, random_state=42\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TF-IDF Vectoriser\n",
                "\n",
                "TF-IDF indicates what the importance of the word is in order to understand the document or dataset. Let us understand with an example. Suppose you have a dataset where students write an essay on the topic, My House. In this dataset, the word a appears many times; it’s a high frequency word compared to other words in the dataset. The dataset contains other words like home, house, rooms and so on that appear less often, so their frequency are lower and they carry more information compared to the word. This is the intuition behind TF-IDF.\n",
                "\n",
                "TF-IDF Vectoriser converts a collection of raw documents to a matrix of TF-IDF features. The Vectoriser is usually trained on only the X_train dataset.\n",
                "\n",
                "ngram_range is the range of number of words in a sequence. [e.g \"very expensive\" is a 2-gram that is considered as an extra feature separately from \"very\" and \"expensive\" when you have a n-gram range of (1,2)]\n",
                "\n",
                "max_features specifies the number of features to consider. [Ordered by feature frequency across the corpus]."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "vectoriser = TfidfVectorizer(ngram_range=(1, 2), max_features=500000)\n",
                "vectoriser.fit(X_train)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tranforming the dataset\n",
                "\n",
                "Transforming the X_train and X_test dataset into matrix of TF-IDF Features by using the TF-IDF Vectoriser. This datasets will be used to train the model and test against it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = vectoriser.transform(X_train)\n",
                "X_test = vectoriser.transform(X_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build & Evaluate Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import (\n",
                "    accuracy_score,\n",
                "    f1_score,\n",
                "    precision_score,\n",
                "    recall_score,\n",
                "    roc_auc_score,\n",
                "    confusion_matrix,\n",
                "    classification_report,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import datetime\n",
                "import joblib\n",
                "\n",
                "\n",
                "def model_Evaluate(model):\n",
                "    with wandb.init(\n",
                "        project=\"project\",\n",
                "        config={\"model\": model},\n",
                "        name=f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}_{model}\",\n",
                "    ) as run:\n",
                "        # PREDICT VALUES FROM TEST DATASET\n",
                "        y_pred = model.predict(X_test)\n",
                "\n",
                "        # PRINT THE CLASSIFICATION REPORT\n",
                "        print(classification_report(y_test, y_pred))\n",
                "\n",
                "        # COMPUTE AND PLOT CONFUSION MATRIX\n",
                "        cf_matrix = confusion_matrix(y_test, y_pred)\n",
                "        categories = [\"Negative\", \"Positive\"]\n",
                "        group_names = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"]\n",
                "        group_percentages = [\n",
                "            \"{0:.2%}\".format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)\n",
                "        ]\n",
                "        labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names, group_percentages)]\n",
                "        labels = np.asarray(labels).reshape(2, 2)\n",
                "        sns.heatmap(\n",
                "            cf_matrix,\n",
                "            annot=labels,\n",
                "            cmap=\"Blues\",\n",
                "            fmt=\"\",\n",
                "            xticklabels=categories,\n",
                "            yticklabels=categories,\n",
                "        )\n",
                "        plt.xlabel(\"Predicted values\", fontdict={\"size\": 14}, labelpad=10)\n",
                "        plt.ylabel(\"Actual values\", fontdict={\"size\": 14}, labelpad=10)\n",
                "        plt.title(\"Confusion Matrix\", fontdict={\"size\": 18}, pad=20)\n",
                "\n",
                "        # LOG TO W&B\n",
                "        wandb.log(\n",
                "            {\n",
                "                \"accuracy\": accuracy_score(y_test, y_pred),\n",
                "                \"precision\": precision_score(\n",
                "                    y_test, y_pred, average=\"weighted\", zero_division=1\n",
                "                ),\n",
                "                \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
                "                \"f1\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
                "                \"roc_auc\": roc_auc_score(y_test, y_pred),\n",
                "                \"confusion_matrix\": wandb.Image(plt),\n",
                "            }\n",
                "        )\n",
                "\n",
                "        # LOG MODEL\n",
                "        artifact = wandb.Artifact(\"model\", type=\"model\")\n",
                "        joblib.dump(model, \"model.pkl\")\n",
                "        artifact.add_file(\"model.pkl\")\n",
                "        run.log_artifact(artifact)\n",
                "        os.remove(\"model.pkl\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Bernoulli Naive Bayes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.naive_bayes import BernoulliNB\n",
                "\n",
                "BNBmodel = BernoulliNB(alpha=2)\n",
                "BNBmodel.fit(X_train, y_train)\n",
                "model_Evaluate(BNBmodel)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Linear Support Vector Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.svm import LinearSVC\n",
                "\n",
                "SVCmodel = LinearSVC(dual=False)\n",
                "SVCmodel.fit(X_train, y_train)\n",
                "model_Evaluate(SVCmodel)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "LRmodel = LogisticRegression(C=2, max_iter=1000, n_jobs=-1)\n",
                "LRmodel.fit(X_train, y_train)\n",
                "model_Evaluate(LRmodel)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}